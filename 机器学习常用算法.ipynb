{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5878066",
   "metadata": {},
   "source": [
    "# 回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5835c111",
   "metadata": {},
   "source": [
    "## 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0891c068",
   "metadata": {},
   "source": [
    "### 解析解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6e5943",
   "metadata": {},
   "source": [
    "模型：$\\hat{y}(\\boldsymbol{w}, \\boldsymbol{x}) = w_0 + w_1 x_1 + ... + w_p x_p = \\boldsymbol{x}^T\\boldsymbol{w}, \\boldsymbol{x} = (1,x_1,x_2,\\dotsc,x_p)^T$\n",
    "\n",
    "对于数据矩阵$\\boldsymbol{X} = (\\boldsymbol{x}_1,\\boldsymbol{x}_2,\\dotsc,\\boldsymbol{x}_n)^T = (1,\\boldsymbol{x}^{(1)},\\dotsc, \\boldsymbol{x}^{(p)})$而言，取损失函数为均方误差，则我们要优化：\n",
    "$$\\min_{\\boldsymbol{w}} \\frac{1}{2n}|| \\boldsymbol{X} \\boldsymbol{w} - \\boldsymbol{y}||_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7f0a50",
   "metadata": {},
   "source": [
    "当$X$可逆时，可以求出解析解$\\boldsymbol{w}_{opt} = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{Y}$.\n",
    "\n",
    "另外，也可以用优化算法求解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "521b4418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffc2296e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((442, 10), (442,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "diabetes_X.shape, diabetes_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeb25bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((309, 10), (309,), (133, 10), (133,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(520)\n",
    "# 划分训练集与测试集\n",
    "index1 = np.random.choice(diabetes_X.shape[0],round(diabetes_X.shape[0]*0.7),replace = False)\n",
    "index2 = np.array(list(set(range(diabetes_X.shape[0]))-set(index1)))\n",
    "X_train, X_test, y_train, y_test = (diabetes_X[index1,].copy(), diabetes_X[index2,].copy(), \n",
    "                                    diabetes_y[index1].copy(), diabetes_y[index2].copy())\n",
    "X_train.shape,y_train.shape,X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f38ae9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.52637181e+02, -2.46209538e-01, -2.95443035e+02,  4.76099495e+02,\n",
       "        3.97871650e+02, -1.88230740e+02, -9.99115610e+01, -1.60960325e+02,\n",
       "        2.62672665e+02,  3.71770233e+02,  1.23301671e+02])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接求出的解析解\n",
    "X = np.concatenate((np.ones((X_train.shape[0],1)),X_train),axis = 1) # 增加截距项对应的列\n",
    "w_analy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y_train)\n",
    "w_analy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6bd1e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "def half_mean_squared_error(y,yhat):\n",
    "    return np.mean((y-yhat)**2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "333a6c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1762.5796693482537"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_with_intercept = np.concatenate((np.ones((X_test.shape[0],1)),X_test),axis = 1)\n",
    "ypred = np.dot(X_test_with_intercept, w_analy)\n",
    "half_mean_squared_error(ypred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0e2a22",
   "metadata": {},
   "source": [
    "### 优化算法求解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c268b8",
   "metadata": {},
   "source": [
    "#### 梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "235494a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次循环，参数值为 w = [ 7.58058252e+00  2.81804294e-02  6.81555645e-03  9.23709419e-02\n",
      "  7.35696834e-02  3.30307972e-03  5.89569285e-03 -8.34563226e-02\n",
      "  6.45334982e-02  8.49922060e-02  5.74607017e-02],损失值为 13168.83063717651\n",
      "第10001次循环，参数值为 w = [ 152.83924579   34.02895437 -127.05256085  381.18960598  293.10521909\n",
      "  -30.30104701  -63.53814577 -218.70391321  152.53871819  282.20840365\n",
      "  157.61287028],损失值为 1393.8152006429543\n",
      "第20001次循环，参数值为 w = [ 152.70195529   11.25607372 -222.1676383   453.53888563  350.162037\n",
      "  -68.04826278 -118.73283879 -239.21888505  155.19111715  328.86012402\n",
      "  149.63918086],损失值为 1338.5315265229856\n",
      "第30001次循环，参数值为 w = [ 152.66047046    3.19613619 -262.62486891  472.36328819  372.11207184\n",
      "  -79.0192716  -136.49237837 -243.49794476  162.79143672  345.56446104\n",
      "  139.8659575 ],损失值为 1331.1553504982137\n",
      "第40001次循环，参数值为 w = [ 152.64698248    0.59383743 -279.62865081  476.20120728  382.33153341\n",
      "  -83.26181755 -143.33758053 -243.53332089  171.10699209  351.44925921\n",
      "  133.38427982],损失值为 1329.8343304755617\n",
      "第50001次循环，参数值为 w = [ 1.52642401e+02 -1.98388583e-01 -2.87046963e+02  4.76179440e+02\n",
      "  3.87712007e+02 -8.56204504e+01 -1.46734435e+02 -2.41852630e+02\n",
      "  1.78694589e+02  3.53098461e+02  1.29569124e+02],损失值为 1329.464531601059\n"
     ]
    }
   ],
   "source": [
    "# 梯度下降法\n",
    "class GD:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, params, grads):\n",
    "        params -= self.lr * grads\n",
    "# 计算\n",
    "w = np.zeros(X.shape[1])\n",
    "l = [None]*50002\n",
    "l[0] = 0\n",
    "optimizer = GD(lr = 0.05)\n",
    "n = X.shape[0]\n",
    "for i in range(50001):\n",
    "    grads = X.T.dot(X.dot(w)-y_train)/n\n",
    "    optimizer.update(w,grads)\n",
    "    l[i+1] = half_mean_squared_error(X.dot(w),y_train)\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"第{i+1}次循环，参数值为 w = {w},损失值为 {l[i+1]}\")\n",
    "    if abs(l[i+1]-l[i])<1e-5:\n",
    "        print(f\"第{i+1}次循环，参数值为 w = {w},损失值为 {l[i+1]}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23822039",
   "metadata": {},
   "source": [
    "下降得非常慢，需要其他加速的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281a798",
   "metadata": {},
   "source": [
    "#### Adagrad算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33c20024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad算法\n",
    "class Adagrad:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "    def update(self,params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = np.zeros_like(grads)\n",
    "        self.h = grads * grads\n",
    "        params -= self.lr * grads/(np.sqrt(self.h)+1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1a76379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次循环，参数值为 w = [ 0.1         0.09999998  0.09999993  0.09999999  0.09999999  0.09999985\n",
      "  0.09999992 -0.09999999  0.09999999  0.09999999  0.09999999],损失值为 14273.978897622457\n",
      "第3001次循环，参数值为 w = [ 152.49976475   24.70584239 -253.49964443  300.09996473  300.09995025\n",
      "   26.07494417 -224.89848863 -298.50072491  227.10053604  300.09992983\n",
      "  188.70530711],损失值为 1375.83291560383\n",
      "第5078次循环，参数值为 w = [ 1.52600765e+02 -2.88960995e-01 -2.95183540e+02  4.76597671e+02\n",
      "  3.97587293e+02 -1.30949242e+02 -1.45689524e+02 -1.85780528e+02\n",
      "  2.56322022e+02  3.51029403e+02  1.23535718e+02],损失值为 1328.561658537623\n"
     ]
    }
   ],
   "source": [
    "# 计算\n",
    "w = np.zeros(X.shape[1])\n",
    "l = [None]*10002\n",
    "l[0] = 0\n",
    "optimizer = Adagrad(lr = 0.1)\n",
    "n = X.shape[0]\n",
    "for i in range(10001):\n",
    "    grads = X.T.dot(X.dot(w)-y_train)/n\n",
    "    optimizer.update(w,grads)\n",
    "    l[i+1] = half_mean_squared_error(X.dot(w),y_train)\n",
    "    if i % 3000 == 0:\n",
    "        print(f\"第{i+1}次循环，参数值为 w = {w},损失值为 {l[i+1]}\")\n",
    "    if abs(l[i+1]-l[i])<1e-5:\n",
    "        print(f\"第{i+1}次循环，参数值为 w = {w},损失值为 {l[i+1]}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec664a84",
   "metadata": {},
   "source": [
    "很快就收敛了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595674ce",
   "metadata": {},
   "source": [
    "#### Adam算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a91d757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam算法\n",
    "class Adam:\n",
    "    def __init__(self, lr = 0.002, beta1 = 0.9, beta2 = 0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.um = None\n",
    "        self.uv = None\n",
    "        self.iter = 0\n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(grads)\n",
    "            self.v = np.zeros_like(grads)\n",
    "            self.um = np.zeros_like(grads)\n",
    "            self.uv = np.zeros_like(grads)\n",
    "        self.iter += 1\n",
    "        self.m = self.beta1 * self.m + (1.0 - self.beta1) * grads\n",
    "        self.v = self.beta2 * self.v + (1.0 - self.beta2) * grads ** 2\n",
    "        self.um = self.m / (1 - self.beta1 ** self.iter)\n",
    "        self.uv = self.v / (1 - self.beta2 ** self.iter)\n",
    "        params -= self.lr * self.um / (np.sqrt(self.uv) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c786c8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次循环，参数值为 w = [ 99.99999993  99.99998226  99.99992664  99.99999459  99.9999932\n",
      "  99.99984863  99.99991519 -99.99999401  99.99999225  99.99999412\n",
      "  99.9999913 ],损失值为 3424.270114090812\n",
      "第51次循环，参数值为 w = [ 144.34745848   -1.63344851 -279.36689667  461.97751198  399.55927795\n",
      " -105.63342143 -160.68962894 -194.64441739  249.76698037  318.20300525\n",
      "  103.9331644 ],损失值为 1365.0407525123767\n",
      "第101次循环，参数值为 w = [ 153.22237653   -2.26634313 -295.2089584   476.57227615  399.0996865\n",
      " -148.8505582  -129.04881891 -177.19192634  257.39052996  357.78952428\n",
      "  124.44569058],损失值为 1328.6977645223342\n",
      "第151次循环，参数值为 w = [ 1.52613979e+02 -3.16814587e-01 -2.95591080e+02  4.76313174e+02\n",
      "  3.97827603e+02 -1.76697619e+02 -1.09291863e+02 -1.66118047e+02\n",
      "  2.61340005e+02  3.67569675e+02  1.23488612e+02],损失值为 1328.5050320441696\n",
      "第179次循环，参数值为 w = [ 1.52627004e+02 -2.83394285e-01 -2.95423312e+02  4.76123390e+02\n",
      "  3.97875814e+02 -1.83095385e+02 -1.03979719e+02 -1.63211065e+02\n",
      "  2.62061767e+02  3.69860414e+02  1.23295711e+02],损失值为 1328.5027223899747\n"
     ]
    }
   ],
   "source": [
    "# 计算\n",
    "w = np.zeros(X.shape[1])\n",
    "l = [None]*10002\n",
    "l[0] = 0\n",
    "optimizer = Adam(lr = 100)\n",
    "n = X.shape[0]\n",
    "for i in range(10001):\n",
    "    grads = X.T.dot(X.dot(w)-y_train)/n\n",
    "    optimizer.update(w,grads)\n",
    "    l[i+1] = half_mean_squared_error(X.dot(w),y_train)\n",
    "    if i % 50 == 0:\n",
    "        print(f\"第{i+1}次循环，参数值为 w = {w},损失值为 {l[i+1]}\")\n",
    "    if abs(l[i+1]-l[i])<1e-5:\n",
    "        print(f\"第{i+1}次循环，参数值为 w = {w},损失值为 {l[i+1]}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288c8a2c",
   "metadata": {},
   "source": [
    "收敛速度特别快。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7776227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 封装为类\n",
    "class LinearReg:\n",
    "    def __init__(self, Y, X, optimizer = Adam, max_iter = 1000, tol = 1e-5):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n, self.n_feature = X.shape\n",
    "        self.optimizer = optimizer\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.center, self.scale, self.Xs, self.center_y, self.ys = self.standardized()\n",
    "        self.beta = np.zeros(self.n_feature)\n",
    "    def standardized(self):\n",
    "        center = np.mean(self.X,axis = 0)\n",
    "        scale = np.std(self.X,axis = 0)\n",
    "        Xs = (self.X-center)/scale\n",
    "        center_y = np.mean(self.Y)\n",
    "        ys = self.Y - center_y\n",
    "        return (center, scale, Xs, center_y, ys)\n",
    "    @staticmethod\n",
    "    def L2_norm(x,y):\n",
    "        return np.sqrt(np.sum((x-y)**2))\n",
    "    def fit(self):\n",
    "        for i in range(self.max_iter):\n",
    "            beta_old = self.beta.copy()\n",
    "            grads = self.Xs.T.dot(self.Xs.dot(self.beta)-self.ys)/self.n\n",
    "            self.optimizer.update(self.beta, grads)\n",
    "            if self.L2_norm(self.beta, beta_old) < self.tol:\n",
    "                self.beta = self.unstandardized()\n",
    "                break\n",
    "    def unstandardized(self):\n",
    "        beta = np.zeros(self.n_feature+1)\n",
    "        beta[0] = self.center_y - np.sum(self.beta/self.scale*self.center)\n",
    "        beta[1:] = self.beta/self.scale\n",
    "        return beta\n",
    "    def predict(self, newdata):\n",
    "        return np.dot(newdata, self.beta[1:]) + self.beta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7700d9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1762.5798759097024"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = Adam(lr=10)\n",
    "linear_reg = LinearReg(Y = y_train,\n",
    "                       X = X_train,\n",
    "                       optimizer = adam)\n",
    "linear_reg.fit()\n",
    "half_mean_squared_error(linear_reg.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5876248",
   "metadata": {},
   "source": [
    "## 带正则项的线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3c3230",
   "metadata": {},
   "source": [
    "### lasso-L1正则"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fec418",
   "metadata": {},
   "source": [
    "在最小二乘回归的损失函数的基础上添加 $L_1$ 正则项，即$\\min_{\\boldsymbol{w}} { \\frac{1}{2n} ||\\boldsymbol{X} \\boldsymbol{w} - \\boldsymbol{y}||_2 ^ 2 + \\alpha ||\\boldsymbol{w}||_1}$，在$X$不是列正交的时候，无法求得解析解。一般使用优化算法如坐标下降法求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1fd33d",
   "metadata": {},
   "source": [
    "![avatar](./images/lasso.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17d311d",
   "metadata": {},
   "source": [
    "算法如下：\n",
    "![avatar](./images/lasso_coordinate_descent.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b48b425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次循环，参数值为 w = [ 153.14056106  302.10730732   22.00102648  826.51095568  303.88606276\n",
      "  -87.54631139  -19.92328089 -258.3364583    64.06797214  166.45880937\n",
      "  -39.70688438],损失值为 1873.7173863947012\n",
      "第201次循环，参数值为 w = [ 152.649404      0.         -290.91769361  475.35309925  395.41293111\n",
      " -155.91347957 -116.34019075 -178.3783239   246.68311849  362.51735282\n",
      "  121.74201768],损失值为 1578.1697360447038\n",
      "第401次循环，参数值为 w = [ 152.64865235    0.         -290.92566999  475.33907579  395.42106475\n",
      " -158.15236287 -114.55410203 -177.42386119  246.89874089  363.33365394\n",
      "  121.74353953],损失值为 1578.2206650439953\n",
      "第601次循环，参数值为 w = [ 152.64862199    0.         -290.92599222  475.33850926  395.42139333\n",
      " -158.24280983 -114.48194718 -177.38530257  246.90745165  363.36663108\n",
      "  121.74360101],损失值为 1578.222726203217\n",
      "第801次循环，参数值为 w = [ 152.64862076    0.         -290.92600524  475.33848638  395.4214066\n",
      " -158.24646372 -114.47903225 -177.38374487  246.90780355  363.36796329\n",
      "  121.7436035 ],损失值为 1578.2228094764996\n",
      "第1001次循环，参数值为 w = [ 152.64862071    0.         -290.92600576  475.33848545  395.42140714\n",
      " -158.24661134 -114.4789145  -177.38368195  246.90781777  363.36801711\n",
      "  121.7436036 ],损失值为 1578.222812840604\n",
      "第1197次循环，参数值为 w = [ 152.64862071    0.         -290.92600579  475.33848542  395.42140716\n",
      " -158.24661728 -114.47890975 -177.38367941  246.90781834  363.36801928\n",
      "  121.7436036 ],损失值为 1578.2228129761288\n"
     ]
    }
   ],
   "source": [
    "#### 坐标下降法解lasso\n",
    "# X 中心标准化，注意因为要进行中心标准化，所以不需要添加截距项\n",
    "center = np.mean(X_train,axis = 0)\n",
    "scale = np.std(X_train,axis = 0)\n",
    "Xs = (X_train-center)/scale\n",
    "# y 中心化\n",
    "center_y = np.mean(y_train)\n",
    "ys = y_train - center_y\n",
    "# 定义软阈值函数\n",
    "def soft_threshold(z,l):\n",
    "    if z > l:\n",
    "        return z - l\n",
    "    elif z < - l:\n",
    "        return z + l\n",
    "    else:\n",
    "        return 0\n",
    "# 初始化参数与残差\n",
    "n,p = Xs.shape\n",
    "w = np.zeros(p)\n",
    "r = ys.copy() # 注意深浅拷贝，当后续不会对此值进行更改时，浅拷贝没问题，但如果会修改这个值，需要深拷贝\n",
    "# 设置惩罚系数\n",
    "lambda_ = 0.1\n",
    "# 记录损失函数变化\n",
    "l = [None] * 2002\n",
    "l[0] = 0\n",
    "# 坐标下降\n",
    "for i in range(2001):\n",
    "    for j in range(p):\n",
    "        z = np.dot(Xs[:,j],r)/n + w[j]\n",
    "        delta = soft_threshold(z,lambda_) - w[j]\n",
    "        w[j] += delta\n",
    "        r -= np.dot(Xs[:,j], delta)\n",
    "    w_lasso = np.zeros(X.shape[1])\n",
    "    w_lasso[0] = center_y - w.dot(center/scale)\n",
    "    w_lasso[1:] = w/scale\n",
    "    l[i+1] = half_mean_squared_error(X.dot(w_lasso),y_train)+lambda_*np.sum(np.abs(w_lasso))\n",
    "    if i % 200 == 0:\n",
    "        print(f\"第{i+1}次循环，参数值为 w = {w_lasso},损失值为 {l[i+1]}\")\n",
    "    if abs(l[i+1]-l[i])<1e-10:\n",
    "        print(f\"第{i+1}次循环，参数值为 w = {w_lasso},损失值为 {l[i+1]}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07db4363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1760.9303711708997"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在测试集上的损失\n",
    "half_mean_squared_error(X_test_with_intercept.dot(w_lasso),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8a3c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 封装为类\n",
    "class Lasso:\n",
    "    def __init__(self, Y, X, lambda_, max_iter = 1000, tol = 1e-4):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n, self.n_feature = X.shape\n",
    "        self.lambda_ = lambda_\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.beta = np.zeros(self.n_feature)\n",
    "        self.center, self.scale, self.Xs, self.center_y, self.ys = self.standardized()\n",
    "    def standardized(self):\n",
    "        center = np.mean(self.X,axis = 0)\n",
    "        scale = np.std(self.X,axis = 0)\n",
    "        Xs = (self.X-center)/scale\n",
    "        center_y = np.mean(self.Y)\n",
    "        ys = self.Y - center_y\n",
    "        return (center, scale, Xs, center_y, ys)\n",
    "    def unstandardized(self):\n",
    "        beta = np.zeros(self.n_feature+1)\n",
    "        beta[0] = self.center_y - np.sum(self.beta/self.scale*self.center)\n",
    "        beta[1:] = self.beta/self.scale\n",
    "        return beta\n",
    "    @staticmethod\n",
    "    def L2_norm(x):\n",
    "        return np.sqrt(np.sum(x**2))\n",
    "    def soft_threshold(self, z):\n",
    "        if z < -self.lambda_:\n",
    "            return z + self.lambda_\n",
    "        elif z > self.lambda_:\n",
    "            return z - self.lambda_\n",
    "        else:\n",
    "            return 0\n",
    "    def fit(self):\n",
    "        r = self.Y.copy()\n",
    "        for i in range(self.max_iter):\n",
    "            beta = self.beta.copy() # 用于计算更新大小\n",
    "            for j in range(self.n_feature):\n",
    "                z = np.dot(self.Xs[:,j],r)/self.n + self.beta[j]\n",
    "                delta = self.soft_threshold(z) - self.beta[j]\n",
    "                self.beta[j] += delta\n",
    "                r -= np.dot(self.Xs[:,j], delta)\n",
    "            if self.L2_norm(self.beta - beta) < self.tol:\n",
    "                self.beta = self.unstandardized()\n",
    "                break\n",
    "    def predict(self, newdata):\n",
    "        return np.dot(newdata, self.beta[1:]) + self.beta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24f63414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1755.9332864742607"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso = Lasso(Y = y_train,\n",
    "              X = X_train,\n",
    "              lambda_ = 0.5,\n",
    "              max_iter = 10000)\n",
    "lasso.fit()\n",
    "half_mean_squared_error(lasso.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed445765",
   "metadata": {},
   "source": [
    "把第二个系数惩罚到了0的效果还是不错的，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004223ea",
   "metadata": {},
   "source": [
    "### 岭回归-L2正则"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d853ecd4",
   "metadata": {},
   "source": [
    "在最小二乘回归的损失函数的基础上添加$L_2$正则项，即$\\min_{\\boldsymbol{w}} \\frac{1}{2n}|| \\boldsymbol{X} \\boldsymbol{w} - \\boldsymbol{y}||_2^2 + \\alpha ||\\boldsymbol{w}||_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ff7dd",
   "metadata": {},
   "source": [
    "可以直接计算出解析解：$\\boldsymbol{w}_{opt} = (\\boldsymbol{X}^T\\boldsymbol{X}+2n\\alpha I)^{-1}\\boldsymbol{X}^T\\boldsymbol{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9423d8c6",
   "metadata": {},
   "source": [
    "为了让惩罚能够公平分配，可以事先对 𝑋 进行中心标准化：\n",
    "![avatar](./images/center_scale.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "704a92f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.52641748e+02, -1.41409472e-01, -2.94539037e+02,  4.75592857e+02,\n",
       "        3.97123753e+02, -1.71924571e+02, -1.11537838e+02, -1.68691304e+02,\n",
       "        2.58964637e+02,  3.66072891e+02,  1.23512050e+02])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X 中心标准化\n",
    "center = np.mean(X_train,axis = 0)\n",
    "scale = np.std(X_train,axis = 0)\n",
    "center,scale\n",
    "Xs = (X_train-center)/scale\n",
    "center_y = np.mean(y_train)\n",
    "# y 中心化\n",
    "ys = y_train - center_y\n",
    "# 岭回归\n",
    "I = np.diag(np.ones((Xs.shape[1],)))\n",
    "alpha = 1/(4*n)\n",
    "w_tilde = np.linalg.inv(Xs.T.dot(Xs)+2 * n * alpha * I).dot(Xs.T).dot(ys)\n",
    "# 解中心化\n",
    "w_ridge = np.zeros(X.shape[1])\n",
    "w_ridge[0] = center_y - w_tilde.dot(center/scale)\n",
    "w_ridge[1:] = w_tilde/scale # 注意[1:-1]取不到最后一个，左闭右开\n",
    "w_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "070b5533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1763.462583930102"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half_mean_squared_error(y_test,X_test_with_intercept.dot(w_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ecfbb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 封装为类\n",
    "class Ridge:\n",
    "    def __init__(self, Y, X, lambda_):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n, self.n_feature = X.shape\n",
    "        self.lambda_ = lambda_\n",
    "        self.center, self.scale, self.Xs, self.center_y, self.ys = self.standardized()\n",
    "        self.beta = np.zeros(self.n_feature)\n",
    "    def standardized(self):\n",
    "        center = np.mean(self.X,axis = 0)\n",
    "        scale = np.std(self.X,axis = 0)\n",
    "        Xs = (self.X-center)/scale\n",
    "        center_y = np.mean(self.Y)\n",
    "        ys = self.Y - center_y\n",
    "        return (center, scale, Xs, center_y, ys)\n",
    "    def fit(self):\n",
    "        I = np.diag(np.ones(self.n_feature))\n",
    "        self.beta = (np.linalg.inv(self.Xs.T.dot(self.Xs)+2 * self.n * self.lambda_ * I)\n",
    "                      .dot(self.Xs.T).dot(self.ys))\n",
    "        self.beta = self.unstandardized()\n",
    "    def unstandardized(self):\n",
    "        beta = np.zeros(self.n_feature+1)\n",
    "        beta[0] = self.center_y - np.sum(self.beta/self.scale*self.center)\n",
    "        beta[1:] = self.beta/self.scale\n",
    "        return beta\n",
    "    def predict(self, newdata):\n",
    "        return np.dot(newdata, self.beta[1:]) + self.beta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c908eeb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1762.7157154890517"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = Ridge(Y = y_train,\n",
    "              X = X_train,\n",
    "              lambda_ = 0.0001)\n",
    "ridge.fit()\n",
    "half_mean_squared_error(ridge.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1686fe3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.52637850e+02, -2.33633967e-01, -2.95329816e+02,  4.76039266e+02,\n",
       "        3.97777896e+02, -1.85874195e+02, -1.01621410e+02, -1.62064138e+02,\n",
       "        2.62177007e+02,  3.70942023e+02,  1.23328645e+02])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae9847",
   "metadata": {},
   "source": [
    "岭回归没有将系数收缩到0的能力，只能将全部系数都收缩一部分，用处并不大。感觉最大的作用是为了让数据阵非奇异便于显式求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e516cd4a",
   "metadata": {},
   "source": [
    "### 弹性网-L1正则和L2正则混合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b9bf5a",
   "metadata": {},
   "source": [
    "在最小二乘回归的损失函数的基础上同时添加 $L_1$ 和 $L_2$ 正则项，$\\min_{\\boldsymbol{w}} { \\frac{1}{2n}} ||\\boldsymbol{X} \\boldsymbol{w} - \\boldsymbol{y}||_2 ^ 2 + \\alpha \\rho ||\\boldsymbol{w}||_1 +\n",
    "\\frac{\\alpha(1-\\rho)}{2} ||\\boldsymbol{w}||_2 ^ 2$，可以用坐标下降法求得最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8381eb",
   "metadata": {},
   "source": [
    "![avatar](./images/elastic_net.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8347600d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次循环，参数值为 w = [ 153.1428127   299.32083265   22.4564557   818.96573439  304.92102262\n",
      "  -84.44412477  -20.22032387 -258.74452497   65.27810477  166.332269\n",
      "  -37.90277575],损失值为 6770.413788018659\n",
      "第301次循环，参数值为 w = [ 152.65825068    0.         -286.09518234  471.93360217  391.55225975\n",
      " -119.9295747  -138.07371445 -196.69021498  233.76914824  350.2521653\n",
      "  123.09819985],损失值为 5279.592146294225\n",
      "第601次循环，参数值为 w = [ 152.65825052    0.         -286.09518391  471.93360045  391.55226162\n",
      " -119.92999878 -138.07338089 -196.69003474  233.76918927  350.25231843\n",
      "  123.09820158],损失值为 5279.59248626117\n",
      "第627次循环，参数值为 w = [ 152.65825052    0.         -286.09518391  471.93360045  391.55226162\n",
      " -119.92999878 -138.07338088 -196.69003474  233.76918927  350.25231843\n",
      "  123.09820158],损失值为 5279.592486265368\n"
     ]
    }
   ],
   "source": [
    "#### 坐标下降法解弹性网\n",
    "# X 中心标准化\n",
    "center = np.mean(X_train,axis = 0)\n",
    "scale = np.std(X_train,axis = 0)\n",
    "Xs = (X_train-center)/scale\n",
    "# y 中心化\n",
    "center_y = np.mean(y_train)\n",
    "ys = y_train - center_y\n",
    "# 定义软阈值函数\n",
    "def soft_threshold(z,l):\n",
    "    if z > l:\n",
    "        return z - l\n",
    "    elif z < - l:\n",
    "        return z + l\n",
    "    else:\n",
    "        return 0\n",
    "# 初始化参数与残差\n",
    "n,p = Xs.shape\n",
    "w = np.zeros(p)\n",
    "w_elanet = np.zeros(X.shape[1])\n",
    "r = ys.copy()\n",
    "# 设置超参数\n",
    "lambda_ = 0.1\n",
    "rho = 0.9\n",
    "# 记录损失函数变化\n",
    "l = [None] * 2002\n",
    "l[0] = 0\n",
    "# 坐标下降\n",
    "for i in range(2001):\n",
    "    for j in range(p):\n",
    "        z = np.dot(Xs[:,j],r)/n + w[j]\n",
    "        delta = soft_threshold(z,rho * lambda_)/((1 - rho) * lambda_ + 1) - w[j]\n",
    "        w[j] += delta\n",
    "        r -= np.dot(Xs[:,j], delta)\n",
    "    w_elanet[0] = center_y - w.dot(center/scale)\n",
    "    w_elanet[1:] = w/scale\n",
    "    l[i+1] = (half_mean_squared_error(X.dot(w_elanet),y_train) + rho*lambda_*np.sum(np.abs(w_elanet)) +\n",
    "              (1-rho)/2*lambda_*np.sum(w_elanet**2))\n",
    "    if i % 300 == 0:\n",
    "        print(f\"第{i+1}次循环，参数值为 w = {w_elanet},损失值为 {l[i+1]}\")\n",
    "    if abs(l[i+1]-l[i])<1e-10:\n",
    "        print(f\"第{i+1}次循环，参数值为 w = {w_elanet},损失值为 {l[i+1]}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2173617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1762.2203135775587"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试集损失\n",
    "half_mean_squared_error(X_test_with_intercept.dot(w_elanet),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7d2c9f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 封装为类\n",
    "class ElasticNet:\n",
    "    def __init__(self, Y, X, lambda_, rho, max_iter = 1000, tol = 1e-4):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n, self.n_feature = X.shape\n",
    "        self.lambda_ = lambda_\n",
    "        self.rho = rho\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.beta = np.zeros(self.n_feature)\n",
    "        self.center, self.scale, self.Xs, self.center_y, self.ys = self.standardized()\n",
    "    def standardized(self):\n",
    "        center = np.mean(self.X,axis = 0)\n",
    "        scale = np.std(self.X,axis = 0)\n",
    "        Xs = (self.X-center)/scale\n",
    "        center_y = np.mean(self.Y)\n",
    "        ys = self.Y - center_y\n",
    "        return (center, scale, Xs, center_y, ys)\n",
    "    def unstandardized(self):\n",
    "        beta = np.zeros(self.n_feature+1)\n",
    "        beta[0] = self.center_y - np.sum(self.beta/self.scale*self.center)\n",
    "        beta[1:] = self.beta/self.scale\n",
    "        return beta\n",
    "    @staticmethod\n",
    "    def L2_norm(x):\n",
    "        return np.sqrt(np.sum(x**2))\n",
    "    @staticmethod\n",
    "    def soft_threshold(z, lambda_):\n",
    "        if z < lambda_:\n",
    "            return z + lambda_\n",
    "        elif z > lambda_:\n",
    "            return z - lambda_\n",
    "        else:\n",
    "            return 0\n",
    "    def fit(self):\n",
    "        r = self.Y.copy()\n",
    "        for i in range(self.max_iter):\n",
    "            beta = self.beta.copy() # 用于计算更新大小\n",
    "            for j in range(self.n_feature):\n",
    "                z = np.dot(self.Xs[:,j],r)/self.n + self.beta[j]\n",
    "                delta = (self.soft_threshold(z, self.rho * self.lambda_)\n",
    "                         /((1 - self.rho) * self.lambda_ + 1) - self.beta[j])\n",
    "                self.beta[j] += delta\n",
    "                r -= np.dot(self.Xs[:,j], delta)\n",
    "            if self.L2_norm(self.beta - beta) < self.tol:\n",
    "                self.beta = self.unstandardized()\n",
    "                break\n",
    "    def predict(self, newdata):\n",
    "        return np.dot(newdata, self.beta[1:]) + self.beta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e8bdd0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1762.3624533635073"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elanet = ElasticNet(Y = y_train,\n",
    "                    X = X_train,\n",
    "                    lambda_ = 0.1,\n",
    "                    rho = 0.9,\n",
    "                    max_iter = 10000)\n",
    "elanet.fit()\n",
    "half_mean_squared_error(elanet.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dad27a",
   "metadata": {},
   "source": [
    "感觉弹性网并不会比lasso好，大系数并不需要收缩，只需要收缩小系数。而岭回归按比例收缩所有系数并不合理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f83159",
   "metadata": {},
   "source": [
    "### MCP惩罚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c1861",
   "metadata": {},
   "source": [
    "非凹惩罚，解决了lasso等惩罚项对不需要收缩的系数带来有偏性的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9694d1f9",
   "metadata": {},
   "source": [
    "![avatar](./images/MCP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd15486f",
   "metadata": {},
   "source": [
    "可以看到，当系数超过某个值时，它将会与没有惩罚时的结果一样（当然，这只是一维的情况，只能说这个惩罚改善了有偏的程度）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "50a1213b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.359322926513111"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MCP 的推荐值\n",
    "2/(1-np.max(X.T.dot(X)-np.diag(np.diag(X.T.dot(X)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "756d019b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次循环，参数值为 w = [ 153.26202658  304.17464389    0.          830.35875114  309.08565146\n",
      "  -61.97987419   -3.84910694 -265.98744374    0.          175.54115202\n",
      "    0.        ],损失值为 1707.507800391112\n",
      "第38次循环，参数值为 w = [ 152.64035268    0.         -283.5273765   471.9887538   395.63734044\n",
      "    0.         -108.29805955 -365.55559342    0.          372.55173407\n",
      "   90.31263603],损失值为 1424.6629310473581\n"
     ]
    }
   ],
   "source": [
    "#### 坐标下降法解 MCP 惩罚项的损失函数\n",
    "# X 中心标准化\n",
    "center = np.mean(X_train,axis = 0)\n",
    "scale = np.std(X_train,axis = 0)\n",
    "Xs = (X_train-center)/scale\n",
    "# y 中心化\n",
    "center_y = np.mean(y_train)\n",
    "ys = y_train - center_y\n",
    "# 定义软阈值函数\n",
    "def soft_threshold(z,l):\n",
    "    if z > l:\n",
    "        return z - l\n",
    "    elif z < - l:\n",
    "        return z + l\n",
    "    else:\n",
    "        return 0\n",
    "# 定义 MCP 的惩罚函数\n",
    "def MCP(w,lam,gamma):\n",
    "    penalty = np.zeros_like(w)\n",
    "    for i in range(len(w)):\n",
    "        if abs(w[i]) <= gamma * lam:\n",
    "            penalty[i] = lam * w[i] - w[i]**2/(2*gamma)\n",
    "        else:\n",
    "            penalty[i] = gamma * lam ** 2/2\n",
    "    return np.sum(penalty)\n",
    "# 定义 MCP 的阈值函数\n",
    "def firm_threshold(z,l,gamma):\n",
    "    if abs(z) <= gamma * l:\n",
    "        return soft_threshold(z,l)/(1-1/(gamma))\n",
    "    else:\n",
    "        return z\n",
    "# 初始化参数与残差\n",
    "n,p = Xs.shape\n",
    "w = np.zeros(p)\n",
    "w_mcp = np.zeros(X.shape[1])\n",
    "r = ys.copy()\n",
    "# 设置超参数\n",
    "lambda_ = 2\n",
    "gamma = 5.36 # MCP惩罚推荐值的近似值\n",
    "# 记录损失函数变化\n",
    "l = [None] * 1002\n",
    "l[0] = 0\n",
    "# 坐标下降\n",
    "for i in range(1001):\n",
    "    for j in range(p):\n",
    "        z = np.dot(Xs[:,j],r)/n + w[j]\n",
    "        delta = firm_threshold(z, lambda_, gamma) - w[j]\n",
    "        w[j] += delta\n",
    "        r -= np.dot(Xs[:,j], delta)\n",
    "    w_mcp[0] = center_y - w.dot(center/scale)\n",
    "    w_mcp[1:] = w/scale\n",
    "    l[i+1] = half_mean_squared_error(X.dot(w_mcp),y_train) + MCP(w_mcp,lambda_,gamma)\n",
    "    if i % 100 == 0:\n",
    "        print(f\"第{i+1}次循环，参数值为 w = {w_mcp},损失值为 {l[i+1]}\")\n",
    "    if abs(l[i+1]-l[i])<1e-10:\n",
    "        print(f\"第{i+1}次循环，参数值为 w = {w_mcp},损失值为 {l[i+1]}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "29b8b2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1733.6348912070428"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half_mean_squared_error(X_test_with_intercept.dot(w_mcp),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "031b590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 封装为类\n",
    "class MCP:\n",
    "    def __init__(self, Y, X, lambda_, gamma, max_iter = 1000, tol = 1e-4):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n, self.n_feature = X.shape\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.beta = np.zeros(self.n_feature)\n",
    "        self.center, self.scale, self.Xs, self.center_y, self.ys = self.standardized()\n",
    "    def standardized(self):\n",
    "        center = np.mean(self.X,axis = 0)\n",
    "        scale = np.std(self.X,axis = 0)\n",
    "        Xs = (self.X-center)/scale\n",
    "        center_y = np.mean(self.Y)\n",
    "        ys = self.Y - center_y\n",
    "        return (center, scale, Xs, center_y, ys)\n",
    "    def unstandardized(self):\n",
    "        beta = np.zeros(self.n_feature+1)\n",
    "        beta[0] = self.center_y - np.sum(self.beta/self.scale*self.center)\n",
    "        beta[1:] = self.beta/self.scale\n",
    "        return beta\n",
    "    @staticmethod\n",
    "    def L2_norm(x):\n",
    "        return np.sqrt(np.sum(x**2))\n",
    "    @staticmethod\n",
    "    def soft_threshold(z, lambda_):\n",
    "        if z < -lambda_:\n",
    "            return z + lambda_\n",
    "        elif z > lambda_:\n",
    "            return z - lambda_\n",
    "        else:\n",
    "            return 0\n",
    "    def firm_threshold(self, z):\n",
    "        if abs(z) <= self.gamma * self.lambda_:\n",
    "            return self.soft_threshold(z, self.lambda_)/(1-1/(self.gamma))\n",
    "        else:\n",
    "            return z\n",
    "    def fit(self):\n",
    "        r = self.Y.copy()\n",
    "        for i in range(self.max_iter):\n",
    "            beta = self.beta.copy() # 用于计算更新大小\n",
    "            for j in range(self.n_feature):\n",
    "                z = np.dot(self.Xs[:,j],r)/self.n + self.beta[j]\n",
    "                delta = self.firm_threshold(z) - self.beta[j]\n",
    "                self.beta[j] += delta\n",
    "                r -= np.dot(self.Xs[:,j], delta)\n",
    "            if self.L2_norm(self.beta - beta) < self.tol:\n",
    "                self.beta = self.unstandardized()\n",
    "                break\n",
    "    def predict(self, newdata):\n",
    "        return np.dot(newdata, self.beta[1:]) + self.beta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b01e50e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1733.6348267933677"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcp = MCP(Y = y_train,\n",
    "          X = X_train,\n",
    "          lambda_ = 2,\n",
    "          gamma = 5.36,\n",
    "          max_iter = 10000)\n",
    "mcp.fit()\n",
    "half_mean_squared_error(mcp.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d556b9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 152.64035123,    0.        , -283.52711268,  471.9886988 ,\n",
       "        395.6366227 ,    0.        , -108.29877561, -365.55505158,\n",
       "          0.        ,  372.55186116,   90.31334276])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcp.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e120fe21",
   "metadata": {},
   "source": [
    "可以看到，MCP惩罚收缩了三个变量，但预测效果反而更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ca78ef",
   "metadata": {},
   "source": [
    "### SCAD 惩罚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbbf4e5",
   "metadata": {},
   "source": [
    "非凹惩罚，解决了lasso等惩罚项对不需要收缩的系数带来有偏性的问题，和MCP惩罚类似。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5675a244",
   "metadata": {},
   "source": [
    "![avatar](./images/SCAD.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2a36afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次循环，参数值为 w = [ 153.27671749  304.17464389    0.          830.35875114  309.08565146\n",
      "  -55.22351125   -9.22088028 -267.47816827    0.          183.80335675\n",
      "    0.        ],损失值为 1716.5152924010367\n",
      "第43次循环，参数值为 w = [ 152.61947083    0.         -283.67119282  472.45780798  394.01802161\n",
      "    0.         -118.92422474 -365.42325474    0.          373.39026203\n",
      "   97.67026306],损失值为 1413.287260907767\n"
     ]
    }
   ],
   "source": [
    "#### 坐标下降法解 SCAD 惩罚项的损失函数\n",
    "# X 中心标准化\n",
    "center = np.mean(X_train,axis = 0)\n",
    "scale = np.std(X_train,axis = 0)\n",
    "Xs = (X_train-center)/scale\n",
    "# y 中心化\n",
    "center_y = np.mean(y_train)\n",
    "ys = y_train - center_y\n",
    "# 定义软阈值函数\n",
    "def soft_threshold(z,l):\n",
    "    if z > l:\n",
    "        return z - l\n",
    "    elif z < - l:\n",
    "        return z + l\n",
    "    else:\n",
    "        return 0\n",
    "# 定义 SCAD 的惩罚函数\n",
    "def SCAD(w,lam,gamma):\n",
    "    penalty = np.zeros_like(w)\n",
    "    for i in range(len(w)):\n",
    "        if abs(w[i]) <= lam:\n",
    "            penalty[i] = lam * w[i]\n",
    "        elif abs(w[i]) <= gamma * lam:\n",
    "            penalty[i] = (gamma * lam * w[i] - (w[i] ** 2 + lam ** 2)/2)/(gamma - 1)\n",
    "        else:\n",
    "            penalty[i] = lam ** 2 * (gamma + 1) / 2\n",
    "    return np.sum(penalty)\n",
    "# 定义 SCAD 的阈值函数\n",
    "def scad_threshold(z,l,gamma):\n",
    "    if abs(z) <= 2 * l:\n",
    "        return soft_threshold(z,l)\n",
    "    elif abs(z) <= gamma * l:\n",
    "        return soft_threshold(z,gamma*l/(gamma-1))/(1-1/(gamma-1))\n",
    "    else:\n",
    "        return z\n",
    "# 初始化参数与残差\n",
    "n,p = Xs.shape\n",
    "w = np.zeros(p)\n",
    "w_scad = np.zeros(X.shape[1])\n",
    "r = ys.copy()\n",
    "# 设置超参数\n",
    "lambda_ = 2\n",
    "gamma = 3.7 # SCAD惩罚推荐值\n",
    "# 记录损失函数变化\n",
    "l = [None] * 1002\n",
    "l[0] = 0\n",
    "# 坐标下降\n",
    "for i in range(1001):\n",
    "    for j in range(p):\n",
    "        z = np.dot(Xs[:,j],r)/n + w[j]\n",
    "        delta = scad_threshold(z, lambda_, gamma) - w[j]\n",
    "        w[j] += delta\n",
    "        r -= np.dot(Xs[:,j], delta)\n",
    "    w_scad[0] = center_y - w.dot(center/scale)\n",
    "    w_scad[1:] = w/scale\n",
    "    l[i+1] = half_mean_squared_error(X.dot(w_scad),y_train) + SCAD(w_scad,lambda_,gamma)\n",
    "    if i % 100 == 0:\n",
    "        print(f\"第{i+1}次循环，参数值为 w = {w_scad},损失值为 {l[i+1]}\")\n",
    "    if abs(l[i+1]-l[i])<1e-10:\n",
    "        print(f\"第{i+1}次循环，参数值为 w = {w_scad},损失值为 {l[i+1]}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fe2044f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1735.9917114873363"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half_mean_squared_error(X_test_with_intercept.dot(w_scad),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c7b9afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 封装为类\n",
    "class SCAD:\n",
    "    def __init__(self, Y, X, lambda_, gamma = 3.7, max_iter = 1000, tol = 1e-4):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n, self.n_feature = X.shape\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.beta = np.zeros(self.n_feature)\n",
    "        self.center, self.scale, self.Xs, self.center_y, self.ys = self.standardized()\n",
    "    def standardized(self):\n",
    "        center = np.mean(self.X,axis = 0)\n",
    "        scale = np.std(self.X,axis = 0)\n",
    "        Xs = (self.X-center)/scale\n",
    "        center_y = np.mean(self.Y)\n",
    "        ys = self.Y - center_y\n",
    "        return (center, scale, Xs, center_y, ys)\n",
    "    def unstandardized(self):\n",
    "        beta = np.zeros(self.n_feature+1)\n",
    "        beta[0] = self.center_y - np.sum(self.beta/self.scale*self.center)\n",
    "        beta[1:] = self.beta/self.scale\n",
    "        return beta\n",
    "    @staticmethod\n",
    "    def L2_norm(x):\n",
    "        return np.sqrt(np.sum(x**2))\n",
    "    @staticmethod\n",
    "    def soft_threshold(z, lambda_):\n",
    "        if z < -lambda_:\n",
    "            return z + lambda_\n",
    "        elif z > lambda_:\n",
    "            return z - lambda_\n",
    "        else:\n",
    "            return 0\n",
    "    def scad_threshold(self, z):\n",
    "        if abs(z) <= 2 * self.lambda_:\n",
    "            return self.soft_threshold(z, self.lambda_)\n",
    "        elif abs(z) <= self.gamma * self.lambda_:\n",
    "            return self.soft_threshold(z,self.gamma*self.lambda_/(self.gamma-1))/(1-1/(self.gamma-1))\n",
    "        else:\n",
    "            return z\n",
    "    def fit(self):\n",
    "        r = self.Y.copy()\n",
    "        for i in range(self.max_iter):\n",
    "            beta = self.beta.copy() # 用于计算更新大小\n",
    "            for j in range(self.n_feature):\n",
    "                z = np.dot(self.Xs[:,j],r)/self.n + self.beta[j]\n",
    "                delta = self.scad_threshold(z) - self.beta[j]\n",
    "                self.beta[j] += delta\n",
    "                r -= np.dot(self.Xs[:,j], delta)\n",
    "            if self.L2_norm(self.beta - beta) < self.tol:\n",
    "                self.beta = self.unstandardized()\n",
    "                break\n",
    "    def predict(self, newdata):\n",
    "        return np.dot(newdata, self.beta[1:]) + self.beta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a337a3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1735.9917947559345"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scad = SCAD(Y = y_train,\n",
    "            X = X_train,\n",
    "            lambda_ = 2,\n",
    "            max_iter = 10000)\n",
    "scad.fit()\n",
    "half_mean_squared_error(scad.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90149903",
   "metadata": {},
   "source": [
    "和MCP惩罚的效果类似。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff2d7d4",
   "metadata": {},
   "source": [
    "## 树模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c2f365",
   "metadata": {},
   "source": [
    "### CART树"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520506e4",
   "metadata": {},
   "source": [
    "![avatar](./images/decision_tree1.png)\n",
    "![avatar](./images/decision_tree2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c921bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回归树\n",
    "class Tree:\n",
    "    def __init__(self, Y, X, min_node_size, max_node_depth, rule = None):\n",
    "        # 数据存储\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n, self.feature_num = X.shape\n",
    "        # 递归控制\n",
    "        self.min_node_size = min_node_size\n",
    "        self.max_node_depth = max_node_depth\n",
    "        self.n = len(Y)\n",
    "        # 节点分裂信息\n",
    "        self.mse = np.var(Y)\n",
    "        self.best_feature = None\n",
    "        self.best_value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        # 节点预测值\n",
    "        self.yhat = np.mean(Y)\n",
    "        # 字符串记录，可以用来调试\n",
    "        self.rule = rule if rule is not None else \"\"\n",
    "    def split_find(self):\n",
    "        max_gain = 0\n",
    "        best_feature = None\n",
    "        best_value = None\n",
    "        for feature in range(self.feature_num):\n",
    "            grid = np.sort(np.unique(self.X[:,feature]))\n",
    "            for value in grid:\n",
    "                lidx = self.X[:, feature] < value\n",
    "                ridx = self.X[:, feature] >= value\n",
    "                n_left = np.sum(lidx)\n",
    "                n_right = np.sum(ridx)\n",
    "                # 注意这里只是跳出本次循环\n",
    "                if (n_left == 0) or (n_right == 0):\n",
    "                    continue\n",
    "                mse_left = np.var(self.Y[lidx])\n",
    "                mse_right = np.var(self.Y[ridx])\n",
    "                mse_gain = self.mse - (n_left * mse_left + n_right * mse_right) / self.n\n",
    "                if mse_gain > max_gain:\n",
    "                    best_feature = feature\n",
    "                    best_value = value\n",
    "                    max_gain = mse_gain\n",
    "        return (best_feature, best_value)\n",
    "    def grow_tree(self):\n",
    "        if (self.max_node_depth > 0) and (self.min_node_size < self.n):\n",
    "            self.best_feature,self.best_value = self.split_find()\n",
    "            if self.best_feature is not None:\n",
    "                lidx = self.X[:, self.best_feature] < self.best_value\n",
    "                ridx = self.X[:, self.best_feature] >= self.best_value\n",
    "                left = Tree(Y = self.Y[lidx],\n",
    "                            X = self.X[lidx],\n",
    "                            min_node_size = self.min_node_size,\n",
    "                            max_node_depth = self.max_node_depth - 1,\n",
    "                            rule = f\"X{self.best_feature+1} < {self.best_value}\")\n",
    "                left.grow_tree()\n",
    "                right = Tree(Y = self.Y[ridx],\n",
    "                             X = self.X[ridx],\n",
    "                             min_node_size = self.min_node_size,\n",
    "                             max_node_depth = self.max_node_depth - 1,\n",
    "                             rule = f\"X{self.best_feature+1} >= {self.best_value}\")\n",
    "                right.grow_tree()\n",
    "                self.left = left\n",
    "                self.right = right\n",
    "    def predict(self,newdata):\n",
    "        n = newdata.shape[0]\n",
    "        yhat = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            cur_node = self\n",
    "            while (cur_node.max_node_depth > 0) and (cur_node.min_node_size < cur_node.n):\n",
    "                if cur_node.best_feature is None:\n",
    "                    break\n",
    "                if newdata[i,cur_node.best_feature] < cur_node.best_value:\n",
    "                    cur_node = cur_node.left\n",
    "                else:\n",
    "                    cur_node = cur_node.right\n",
    "            yhat[i] = cur_node.yhat\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c9a508c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cart = Tree(Y = y_train,\n",
    "            X = X_train,\n",
    "            min_node_size = 10,\n",
    "            max_node_depth = 5)\n",
    "cart.grow_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6ffd8146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1993.1437245941881"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half_mean_squared_error(cart.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f264052",
   "metadata": {},
   "source": [
    "单棵决策树效果并不怎么样，不过可以通过集成模型来提高预测效果，集成模型有两族，bagging和boosting。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89217d",
   "metadata": {},
   "source": [
    "### bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e7561b",
   "metadata": {},
   "source": [
    "#### 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7879cfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林的基学习器\n",
    "class Tree:\n",
    "    def __init__(self, Y, X, min_node_size, max_node_depth, mtry, rule = None):\n",
    "        # 数据存储\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n, self.feature_num = X.shape\n",
    "        # 递归控制\n",
    "        self.min_node_size = min_node_size\n",
    "        self.max_node_depth = max_node_depth\n",
    "        self.n = len(Y)\n",
    "        # 节点分裂信息\n",
    "        self.mtry = mtry\n",
    "        self.mse = np.var(Y)\n",
    "        self.best_feature = None\n",
    "        self.best_value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        # 节点预测值\n",
    "        self.yhat = np.mean(Y)\n",
    "        # 字符串记录，可以用来调试\n",
    "        self.rule = rule if rule is not None else \"\"\n",
    "    def split_find(self):\n",
    "        max_gain = 0\n",
    "        best_feature = None\n",
    "        best_value = None\n",
    "        feature_try = np.random.choice(np.arange(self.feature_num), self.mtry, replace = False)\n",
    "        for feature in feature_try:\n",
    "            grid = np.sort(np.unique(self.X[:,feature]))\n",
    "            for value in grid:\n",
    "                lidx = self.X[:, feature] < value\n",
    "                ridx = self.X[:, feature] >= value\n",
    "                n_left = np.sum(lidx)\n",
    "                n_right = np.sum(ridx)\n",
    "                # 注意这里只是跳出本次循环\n",
    "                if (n_left == 0) or (n_right == 0):\n",
    "                    continue\n",
    "                mse_left = np.var(self.Y[lidx])\n",
    "                mse_right = np.var(self.Y[ridx])\n",
    "                mse_gain = self.mse - (n_left * mse_left + n_right * mse_right) / self.n\n",
    "                if mse_gain > max_gain:\n",
    "                    best_feature = feature\n",
    "                    best_value = value\n",
    "                    max_gain = mse_gain\n",
    "        return (best_feature, best_value)\n",
    "    def grow_tree(self):\n",
    "        if (self.max_node_depth > 0) and (self.min_node_size < self.n):\n",
    "            self.best_feature,self.best_value = self.split_find()\n",
    "            if self.best_feature is not None:\n",
    "                lidx = self.X[:, self.best_feature] < self.best_value\n",
    "                ridx = self.X[:, self.best_feature] >= self.best_value\n",
    "                left = Tree(Y = self.Y[lidx],\n",
    "                            X = self.X[lidx],\n",
    "                            min_node_size = self.min_node_size,\n",
    "                            max_node_depth = self.max_node_depth - 1,\n",
    "                            mtry = self.mtry,\n",
    "                            rule = f\"X{self.best_feature+1} < {self.best_value}\")\n",
    "                left.grow_tree()\n",
    "                right = Tree(Y = self.Y[ridx],\n",
    "                             X = self.X[ridx],\n",
    "                             min_node_size = self.min_node_size,\n",
    "                             max_node_depth = self.max_node_depth - 1,\n",
    "                             mtry = self.mtry,\n",
    "                             rule = f\"X{self.best_feature+1} >= {self.best_value}\")\n",
    "                right.grow_tree()\n",
    "                self.left = left\n",
    "                self.right = right\n",
    "    def predict(self,newdata):\n",
    "        n = newdata.shape[0]\n",
    "        yhat = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            cur_node = self\n",
    "            while (cur_node.max_node_depth > 0) and (cur_node.min_node_size < cur_node.n):\n",
    "                if cur_node.best_feature is None:\n",
    "                    break\n",
    "                if newdata[i,cur_node.best_feature] < cur_node.best_value:\n",
    "                    cur_node = cur_node.left\n",
    "                else:\n",
    "                    cur_node = cur_node.right\n",
    "            yhat[i] = cur_node.yhat\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1a512e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# 随机森林\n",
    "class RandomForest:\n",
    "    def __init__(self, Y, X, min_node_size, max_node_depth, mtry, ntree):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n, self.feature_num = X.shape\n",
    "        self.min_node_size = min_node_size\n",
    "        self.max_node_depth = max_node_depth\n",
    "        self.mtry = mtry\n",
    "        self.ntree = ntree\n",
    "        self.rf = []\n",
    "    def fit(self):\n",
    "        for i in tqdm(range(self.ntree)):\n",
    "            bootstrap_index = np.random.choice(self.n, self.n, replace = True)\n",
    "            X, Y = self.X[bootstrap_index],self.Y[bootstrap_index]\n",
    "            tree = Tree(Y = Y,\n",
    "                        X = X,\n",
    "                        min_node_size = self.min_node_size,\n",
    "                        max_node_depth = self.max_node_depth,\n",
    "                        mtry = self.mtry)\n",
    "            tree.grow_tree()\n",
    "            self.rf.append(tree)\n",
    "    def predict(self, newdata):\n",
    "        n = newdata.shape[0]\n",
    "        tmp = np.zeros((self.ntree,n))\n",
    "        for i in range(self.ntree):\n",
    "            tmp[i,:] = self.rf[i].predict(newdata)\n",
    "        return np.mean(tmp,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "69a0dc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1732.7680518928373"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(521)\n",
    "rf = RandomForest(Y = y_train,\n",
    "                  X = X_train,\n",
    "                  min_node_size = 10,\n",
    "                  max_node_depth = 4,\n",
    "                  mtry = 4,\n",
    "                  ntree = 100)\n",
    "rf.fit()\n",
    "half_mean_squared_error(rf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c4ce1d",
   "metadata": {},
   "source": [
    "效果比较显著。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf44590b",
   "metadata": {},
   "source": [
    "### boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe830b6",
   "metadata": {},
   "source": [
    "#### GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "71006323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBDT的基学习器，和最开始那棵决策树一样，只不过集成时传的Y是残差\n",
    "class Tree:\n",
    "    def __init__(self, Y, X, min_node_size, max_node_depth, rule = None):\n",
    "        # 数据存储\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n, self.feature_num = X.shape\n",
    "        # 递归控制\n",
    "        self.min_node_size = min_node_size\n",
    "        self.max_node_depth = max_node_depth\n",
    "        self.n = len(Y)\n",
    "        # 节点分裂信息\n",
    "        self.mse = np.var(Y)\n",
    "        self.best_feature = None\n",
    "        self.best_value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        # 节点预测值\n",
    "        self.yhat = np.mean(Y)\n",
    "        # 字符串记录，可以用来调试\n",
    "        self.rule = rule if rule is not None else \"\"\n",
    "    def split_find(self):\n",
    "        max_gain = 0\n",
    "        best_feature = None\n",
    "        best_value = None\n",
    "        for feature in range(self.feature_num):\n",
    "            grid = np.sort(np.unique(self.X[:,feature]))\n",
    "            for value in grid:\n",
    "                lidx = self.X[:, feature] < value\n",
    "                ridx = self.X[:, feature] >= value\n",
    "                n_left = np.sum(lidx)\n",
    "                n_right = np.sum(ridx)\n",
    "                # 注意这里只是跳出本次循环\n",
    "                if (n_left == 0) or (n_right == 0):\n",
    "                    continue\n",
    "                mse_left = np.var(self.Y[lidx])\n",
    "                mse_right = np.var(self.Y[ridx])\n",
    "                mse_gain = self.mse - (n_left * mse_left + n_right * mse_right) / self.n\n",
    "                if mse_gain > max_gain:\n",
    "                    best_feature = feature\n",
    "                    best_value = value\n",
    "                    max_gain = mse_gain\n",
    "        return (best_feature, best_value)\n",
    "    def grow_tree(self):\n",
    "        if (self.max_node_depth > 0) and (self.min_node_size < self.n):\n",
    "            self.best_feature,self.best_value = self.split_find()\n",
    "            if self.best_feature is not None:\n",
    "                lidx = self.X[:, self.best_feature] < self.best_value\n",
    "                ridx = self.X[:, self.best_feature] >= self.best_value\n",
    "                left = Tree(Y = self.Y[lidx],\n",
    "                            X = self.X[lidx],\n",
    "                            min_node_size = self.min_node_size,\n",
    "                            max_node_depth = self.max_node_depth - 1,\n",
    "                            rule = f\"X{self.best_feature+1} < {self.best_value}\")\n",
    "                left.grow_tree()\n",
    "                right = Tree(Y = self.Y[ridx],\n",
    "                             X = self.X[ridx],\n",
    "                             min_node_size = self.min_node_size,\n",
    "                             max_node_depth = self.max_node_depth - 1,\n",
    "                             rule = f\"X{self.best_feature+1} >= {self.best_value}\")\n",
    "                right.grow_tree()\n",
    "                self.left = left\n",
    "                self.right = right\n",
    "    def predict(self,newdata):\n",
    "        n = newdata.shape[0]\n",
    "        yhat = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            cur_node = self\n",
    "            while (cur_node.max_node_depth > 0) and (cur_node.min_node_size < cur_node.n):\n",
    "                if cur_node.best_feature is None:\n",
    "                    break\n",
    "                if newdata[i,cur_node.best_feature] < cur_node.best_value:\n",
    "                    cur_node = cur_node.left\n",
    "                else:\n",
    "                    cur_node = cur_node.right\n",
    "            yhat[i] = cur_node.yhat\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1e8c4d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBDT:\n",
    "    def __init__(self, Y, X, min_node_size, max_node_depth, nboost, lr = 1):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n = X.shape[0]\n",
    "        self.min_node_size = min_node_size\n",
    "        self.max_node_depth = max_node_depth\n",
    "        self.nboost = nboost\n",
    "        self.lr = lr\n",
    "        self.ybar = np.mean(Y)\n",
    "        self.base_pred = np.mean(Y)\n",
    "        self.tree_set = []\n",
    "    def fit(self):\n",
    "        for i in tqdm(range(self.nboost)):\n",
    "            res = self.Y - self.base_pred\n",
    "            tree = Tree(Y = res,\n",
    "                        X = self.X,\n",
    "                        min_node_size = self.min_node_size,\n",
    "                        max_node_depth = self.max_node_depth)\n",
    "            tree.grow_tree()\n",
    "            self.base_pred += self.lr * tree.predict(self.X)\n",
    "            self.tree_set.append(tree)\n",
    "    def predict(self, newdata):\n",
    "        n = newdata.shape[0]\n",
    "        yhat = self.ybar\n",
    "        for tree in self.tree_set:\n",
    "            yhat += self.lr * tree.predict(newdata)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "628bfef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:25<00:00,  3.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1796.1243293687164"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbdt = GBDT(Y = y_train,\n",
    "            X = X_train,\n",
    "            min_node_size = 10,\n",
    "            max_node_depth = 3,\n",
    "            nboost = 100,\n",
    "            lr = 0.05)\n",
    "gbdt.fit()\n",
    "half_mean_squared_error(gbdt.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00986e1b",
   "metadata": {},
   "source": [
    "感觉提升不上去啊。。。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62001554",
   "metadata": {},
   "source": [
    "#### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1118f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGboost的基学习器\n",
    "class Tree:\n",
    "    def __init__(self, X, grad, hess, min_node_size, max_node_depth, mtry, lambda_ = 1, gamma = 1, \n",
    "                 eps = 0.1, rule = None):\n",
    "        self.X = X\n",
    "        self.n, self.feature_num = X.shape\n",
    "        self.grad = grad\n",
    "        self.hess = hess\n",
    "        self.min_node_size = min_node_size\n",
    "        self.max_node_depth = max_node_depth\n",
    "        self.mtry = mtry\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.score = self.compute_score(self.grad, self.hess) #叶节点得分\n",
    "        self.rule = rule if rule is not None else \"\"\n",
    "        self.best_feature = None\n",
    "        self.best_value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.leaf_weight = self.compute_weight(self.grad, self.hess)\n",
    "    def compute_score(self, grad, hess):\n",
    "        return -1/2 * np.sum(grad)**2 / (np.sum(hess) + self.lambda_)\n",
    "    def compute_weight(self, grad, hess):\n",
    "        return - np.sum(grad) / (np.sum(hess) + self.lambda_)\n",
    "    def split_find(self):\n",
    "        max_gain = 0\n",
    "        best_feature = None\n",
    "        best_value = None\n",
    "        # Exact Greedy Algorithm\n",
    "        feature_try = np.random.choice(np.arange(self.feature_num), self.mtry, replace = False)\n",
    "        for feature in feature_try:\n",
    "            grid = np.sort(np.unique(self.X[:,feature]))\n",
    "            for value in grid:\n",
    "                lidx = self.X[:, feature] < value\n",
    "                ridx = self.X[:, feature] >= value\n",
    "                lscore = self.compute_score(self.grad[lidx], self.hess[lidx])\n",
    "                rscore = self.compute_score(self.grad[ridx], self.hess[ridx])\n",
    "                score_gain = (self.score - lscore - rscore)/2 - self.gamma\n",
    "                if score_gain > max_gain:\n",
    "                    best_feature = feature\n",
    "                    best_value = value\n",
    "                    max_gain = score_gain\n",
    "        return (best_feature, best_value)\n",
    "    def grow_tree(self):\n",
    "        if (self.max_node_depth > 0) and (self.min_node_size < self.n):\n",
    "            self.best_feature,self.best_value = self.split_find()\n",
    "            if self.best_feature is not None:\n",
    "                lidx = self.X[:, self.best_feature] < self.best_value\n",
    "                ridx = self.X[:, self.best_feature] >= self.best_value\n",
    "                left = Tree(X = self.X[lidx],\n",
    "                            grad = self.grad[lidx],\n",
    "                            hess = self.hess[lidx],\n",
    "                            min_node_size = self.min_node_size,\n",
    "                            max_node_depth = self.max_node_depth - 1,\n",
    "                            mtry = self.mtry,\n",
    "                            lambda_ = self.lambda_,\n",
    "                            gamma = self.gamma,\n",
    "                            eps = self.eps,\n",
    "                            rule = f\"X{self.best_feature+1} < {self.best_value}\")\n",
    "                left.grow_tree()\n",
    "                right = Tree(X = self.X[ridx],\n",
    "                            grad = self.grad[ridx],\n",
    "                            hess = self.hess[ridx],\n",
    "                            min_node_size = self.min_node_size,\n",
    "                            max_node_depth = self.max_node_depth - 1,\n",
    "                            mtry = self.mtry,\n",
    "                            lambda_ = self.lambda_,\n",
    "                            gamma = self.gamma,\n",
    "                            eps = self.eps,\n",
    "                            rule = f\"X{self.best_feature+1} >= {self.best_value}\")\n",
    "                right.grow_tree()\n",
    "                self.left = left\n",
    "                self.right = right\n",
    "    def predict(self, newdata):\n",
    "        n = newdata.shape[0]\n",
    "        yhat = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            cur_node = self\n",
    "            while (cur_node.max_node_depth > 0) and (cur_node.min_node_size < cur_node.n):\n",
    "                if cur_node.best_feature is None:\n",
    "                    break\n",
    "                if newdata[i,cur_node.best_feature] < cur_node.best_value:\n",
    "                    cur_node = cur_node.left\n",
    "                else:\n",
    "                    cur_node = cur_node.right\n",
    "            yhat[i] = cur_node.leaf_weight\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fb3e4c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "class XgboostRegressor:\n",
    "    def __init__(self, X, Y, min_node_size, max_node_depth, mtry, nboost, lambda_ = 1, gamma = 1, eps = 0.1, lr = 1):\n",
    "        self.X = X\n",
    "        self.n,self.p = X.shape\n",
    "        self.Y = Y\n",
    "        self.min_node_size = min_node_size\n",
    "        self.max_node_depth = max_node_depth\n",
    "        self.mtry = mtry\n",
    "        self.nboost = nboost\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.lr = lr\n",
    "        self.tree_set = []\n",
    "        self.base_pred = np.mean(Y)\n",
    "    \n",
    "    def fit(self):\n",
    "        for i in tqdm(range(self.nboost)):\n",
    "            grad = self.base_pred - self.Y\n",
    "            hess = np.ones_like(grad)\n",
    "            tree = Tree(X = self.X,\n",
    "                        grad = grad,\n",
    "                        hess = hess,\n",
    "                        min_node_size = self.min_node_size,\n",
    "                        max_node_depth = self.max_node_depth,\n",
    "                        mtry = self.mtry,\n",
    "                        lambda_ = self.lambda_,\n",
    "                        gamma = self.gamma,\n",
    "                        eps = self.eps)\n",
    "            tree.grow_tree()\n",
    "            self.base_pred += self.lr * tree.predict(self.X)\n",
    "            self.tree_set.append(tree)\n",
    "    def predict(self,newdata):\n",
    "        n = newdata.shape[0]\n",
    "        yhat = np.ones(n) * np.mean(self.Y)\n",
    "        for tree in self.tree_set:\n",
    "            yhat += self.lr * tree.predict(newdata)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4eef13a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 10.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1697.0190564794805"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(521)\n",
    "xgbooster = XgboostRegressor(X = X_train,\n",
    "                             Y = y_train,\n",
    "                             min_node_size = 10,\n",
    "                             max_node_depth = 4,\n",
    "                             mtry = 4,\n",
    "                             nboost = 100,\n",
    "                             gamma = 1,\n",
    "                             lr = 0.05)\n",
    "xgbooster.fit()\n",
    "half_mean_squared_error(xgbooster.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eec4b6",
   "metadata": {},
   "source": [
    "提上去了！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfdf55f",
   "metadata": {},
   "source": [
    "# 分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a89e7",
   "metadata": {},
   "source": [
    "## 逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d1f7a8",
   "metadata": {},
   "source": [
    "### 不带正则项的逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d375a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam优化算法\n",
    "class Adam:\n",
    "    def __init__(self, lr = 0.002, beta1 = 0.9, beta2 = 0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.um = None\n",
    "        self.uv = None\n",
    "        self.iter = 0\n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(grads)\n",
    "            self.v = np.zeros_like(grads)\n",
    "            self.um = np.zeros_like(grads)\n",
    "            self.uv = np.zeros_like(grads)\n",
    "        self.iter += 1\n",
    "        self.m = self.beta1 * self.m + (1.0 - self.beta1) * grads\n",
    "        self.v = self.beta2 * self.v + (1.0 - self.beta2) * grads ** 2\n",
    "        self.um = self.m / (1 - self.beta1 ** self.iter)\n",
    "        self.uv = self.v / (1 - self.beta2 ** self.iter)\n",
    "        params -= self.lr * self.um / (np.sqrt(self.uv) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d31d0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR:\n",
    "    def __init__(self, Y, X, optimizer = Adam, max_iter = 1000, tol = 1e-5):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n,self.n_feature = X.shape\n",
    "        self.optimizer = optimizer\n",
    "        self.beta = np.zeros(self.n_feature)\n",
    "        self.P = self.sigmoid(self.X,self.beta)\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "    @staticmethod\n",
    "    def sigmoid(x,beta):\n",
    "        return 1/(1+np.exp(-np.dot(x,beta)))\n",
    "    def fit(self):\n",
    "        for i in range(self.max_iter):\n",
    "            grads = np.dot(self.X.T, self.P-self.Y)\n",
    "            beta_old = self.beta.copy()\n",
    "            self.optimizer.update(self.beta, grads)\n",
    "            self.P = self.sigmoid(self.X,self.beta)\n",
    "            if np.sum((self.beta - beta_old)**2) < self.tol:\n",
    "                break\n",
    "    def predict_prob(self,newdata):\n",
    "        return self.sigmoid(newdata, self.beta)\n",
    "    def predict(self,newdata,threshold = 1e-4):\n",
    "        P = self.predict_prob(newdata)\n",
    "        return np.where(P>=threshold+0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20f3eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "X,y = datasets.load_breast_cancer(return_X_y=True)\n",
    "# 划分训练集和测试集\n",
    "np.random.seed(104)\n",
    "index1 = np.random.choice(X.shape[0],round(X.shape[0]*0.7),replace = False)\n",
    "index2 = np.array(list(set(range(X.shape[0]))-set(index1)))\n",
    "X_train, X_test, y_train, y_test = X[index1,].copy(), X[index2,].copy(), y[index1].copy(), y[index2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfcdfe86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9590643274853801"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = Adam(lr=0.1)\n",
    "logistic_reg = LR(Y = y_train,\n",
    "                  X = X_train,\n",
    "                  optimizer=adam)\n",
    "logistic_reg.fit()\n",
    "np.mean(logistic_reg.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d46d1bd",
   "metadata": {},
   "source": [
    "### 带L1惩罚的逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af6c784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_lasso:\n",
    "    def __init__(self, Y, X, max_iter, lambda_, tol = 1e-4):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n, self.n_feature = X.shape\n",
    "        self.max_iter = max_iter\n",
    "        self.lambda_ = lambda_\n",
    "        self.tol = tol\n",
    "        self.center,self.scale,self.Xs = self.standardized()\n",
    "        self.beta = np.zeros(self.n_feature)\n",
    "    def standardized(self):\n",
    "        center = np.mean(self.X,axis = 0)\n",
    "        scale = np.std(self.X,axis = 0)\n",
    "        Xs = (self.X-center)/scale\n",
    "        return (center, scale, Xs)\n",
    "    @staticmethod\n",
    "    def L2_norm(x,y):\n",
    "        return np.sqrt(np.sum((x-y)**2))\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    @staticmethod\n",
    "    def soft_threshold(z, lambda_):\n",
    "        if z < -lambda_:\n",
    "            re = z + lambda_\n",
    "        elif z > lambda_:\n",
    "            re = z - lambda_\n",
    "        else:\n",
    "            re = 0\n",
    "        return re\n",
    "    \n",
    "    def fit(self):\n",
    "        for i in range(self.max_iter):\n",
    "            beta = self.beta.copy()\n",
    "            for j in range(self.n_feature):\n",
    "                P = self.sigmoid(np.dot(self.Xs, beta))\n",
    "                grad = np.dot(self.Xs.T, P - self.Y)\n",
    "                V = np.sum(self.Xs[:,j] * P * (1-P) * self.Xs[:,j])\n",
    "                z = beta[j] - grad[j]/V\n",
    "                beta[j] = self.soft_threshold(z, self.lambda_/V)\n",
    "            if (self.L2_norm(beta,self.beta) > self.tol):\n",
    "                self.beta = beta.copy()\n",
    "            else:\n",
    "                self.beta = self.unstandardized()\n",
    "                break\n",
    "    def unstandardized(self):\n",
    "        beta = np.zeros(self.n_feature+1)\n",
    "        beta[0] = -np.sum(self.center * self.beta / self.scale)\n",
    "        beta[1:] = self.beta/self.scale\n",
    "        return beta\n",
    "    def predict_prob(self,newdata):\n",
    "        tmp = np.dot(newdata, self.beta[1:])+self.beta[0]\n",
    "        return self.sigmoid(tmp)\n",
    "    def predict(self, newdata, threshold = 1e-4):\n",
    "        P = self.predict_prob(newdata)\n",
    "        return np.where(P > (0.5+threshold), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ef09811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824561403508771"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_reg_lasso = LR_lasso(Y = y_train,\n",
    "                              X = X_train,\n",
    "                              max_iter=1000,\n",
    "                              lambda_=5)\n",
    "logistic_reg_lasso.fit()\n",
    "np.mean(logistic_reg_lasso.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3387c50",
   "metadata": {},
   "source": [
    "### 带MCP惩罚的逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ade92e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_MCP:\n",
    "    def __init__(self, Y, X, max_iter, lambda_, gamma, tol = 1e-4):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n, self.n_feature = X.shape\n",
    "        self.max_iter = max_iter\n",
    "        self.lambda_ = lambda_\n",
    "        self.tol = tol\n",
    "        self.gamma = gamma\n",
    "        self.center,self.scale,self.Xs = self.standardized()\n",
    "        self.beta = np.zeros(self.n_feature)\n",
    "    def standardized(self):\n",
    "        center = np.mean(self.X,axis = 0)\n",
    "        scale = np.std(self.X,axis = 0)\n",
    "        Xs = (self.X-center)/scale\n",
    "        return (center, scale, Xs)\n",
    "    @staticmethod\n",
    "    def L2_norm(x,y):\n",
    "        return np.sqrt(np.sum((x-y)**2))\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    @staticmethod\n",
    "    def soft_threshold(z, lambda_):\n",
    "        if z < -lambda_:\n",
    "            re = z + lambda_\n",
    "        elif z > lambda_:\n",
    "            re = z - lambda_\n",
    "        else:\n",
    "            re = 0\n",
    "        return re\n",
    "    def firm_threshold(self, z, v):\n",
    "        if (abs(z) < self.gamma * self.lambda_):\n",
    "            return self.soft_threshold(z, self.lambda_/v)/(1-1/(self.gamma * v))\n",
    "        else:\n",
    "            return z\n",
    "    def fit(self):\n",
    "        for i in range(self.max_iter):\n",
    "            beta = self.beta.copy()\n",
    "            for j in range(self.n_feature):\n",
    "                P = self.sigmoid(np.dot(self.Xs, beta))\n",
    "                grad = np.dot(self.Xs.T, P - self.Y)\n",
    "                V = np.sum(self.Xs[:,j] * P * (1-P) * self.Xs[:,j])\n",
    "                z = beta[j] - grad[j]/V\n",
    "                beta[j] = self.firm_threshold(z, V)\n",
    "            if (self.L2_norm(beta,self.beta) > self.tol):\n",
    "                self.beta = beta.copy()\n",
    "            else:\n",
    "                self.beta = self.unstandardized()\n",
    "                break\n",
    "    def unstandardized(self):\n",
    "        beta = np.zeros(self.n_feature+1)\n",
    "        beta[0] = -np.sum(self.center * self.beta / self.scale)\n",
    "        beta[1:] = self.beta/self.scale\n",
    "        return beta\n",
    "    def predict_prob(self,newdata):\n",
    "        tmp = np.dot(newdata, self.beta[1:])+self.beta[0]\n",
    "        return self.sigmoid(tmp)\n",
    "    def predict(self, newdata, threshold = 1e-4):\n",
    "        P = self.predict_prob(newdata)\n",
    "        return np.where(P > (0.5+threshold), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdbb4adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824561403508771"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_reg_mcp = LR_MCP(Y = y_train,\n",
    "                          X = X_train,\n",
    "                          max_iter = 10000,\n",
    "                          lambda_ = 4,\n",
    "                          gamma = 3)\n",
    "logistic_reg_mcp.fit()\n",
    "np.mean(logistic_reg_mcp.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc3f2c3",
   "metadata": {},
   "source": [
    "### 带SCAD惩罚的逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "752462d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_SCAD:\n",
    "    def __init__(self, Y, X, max_iter, lambda_, gamma, tol = 1e-4):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n, self.n_feature = X.shape\n",
    "        self.max_iter = max_iter\n",
    "        self.lambda_ = lambda_\n",
    "        self.tol = tol\n",
    "        self.gamma = gamma\n",
    "        self.center,self.scale,self.Xs = self.standardized()\n",
    "        self.beta = np.zeros(self.n_feature)\n",
    "    def standardized(self):\n",
    "        center = np.mean(self.X,axis = 0)\n",
    "        scale = np.std(self.X,axis = 0)\n",
    "        Xs = (self.X-center)/scale\n",
    "        return (center, scale, Xs)\n",
    "    @staticmethod\n",
    "    def L2_norm(x,y):\n",
    "        return np.sqrt(np.sum((x-y)**2))\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    @staticmethod\n",
    "    def soft_threshold(z, lambda_):\n",
    "        if z < -lambda_:\n",
    "            re = z + lambda_\n",
    "        elif z > lambda_:\n",
    "            re = z - lambda_\n",
    "        else:\n",
    "            re = 0\n",
    "        return re\n",
    "    def scad_threshold(self, z, v):\n",
    "        if (abs(z) < (1+1/v) * self.lambda_):\n",
    "            return self.soft_threshold(z, self.lambda_/v)\n",
    "        elif (abs(z) < self.gamma * self.lambda_):\n",
    "            tmp = self.soft_threshold(z, self.gamma * self.lambda_/((self.gamma-1)*v))\n",
    "            return tmp/(1-1/((self.gamma-1)*v))\n",
    "        else:\n",
    "            return z\n",
    "    def fit(self):\n",
    "        for i in range(self.max_iter):\n",
    "            beta = self.beta.copy()\n",
    "            for j in range(self.n_feature):\n",
    "                P = self.sigmoid(np.dot(self.Xs, beta))\n",
    "                grad = np.dot(self.Xs.T, P - self.Y)\n",
    "                V = np.sum(self.Xs[:,j] * P * (1-P) * self.Xs[:,j])\n",
    "                z = beta[j] - grad[j]/V\n",
    "                beta[j] = self.scad_threshold(z, V)\n",
    "            if (self.L2_norm(beta,self.beta) > self.tol):\n",
    "                self.beta = beta.copy()\n",
    "            else:\n",
    "                self.beta = self.unstandardized()\n",
    "                break\n",
    "    def unstandardized(self):\n",
    "        beta = np.zeros(self.n_feature+1)\n",
    "        beta[0] = -np.sum(self.center * self.beta / self.scale)\n",
    "        beta[1:] = self.beta/self.scale\n",
    "        return beta\n",
    "    def predict_prob(self,newdata):\n",
    "        tmp = np.dot(newdata, self.beta[1:])+self.beta[0]\n",
    "        return self.sigmoid(tmp)\n",
    "    def predict(self, newdata, threshold = 1e-4):\n",
    "        P = self.predict_prob(newdata)\n",
    "        return np.where(P > (0.5+threshold), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c1d707d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824561403508771"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_reg_scad = LR_SCAD(Y = y_train,\n",
    "                            X = X_train,\n",
    "                            max_iter = 10000,\n",
    "                            lambda_ = 5,\n",
    "                            gamma = 5)\n",
    "logistic_reg_scad.fit()\n",
    "np.mean(logistic_reg_scad.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085c117",
   "metadata": {},
   "source": [
    "## 支持向量机"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f8724",
   "metadata": {},
   "source": [
    "这个SMO算法写得实在是不怎么样，所以效果可能也不怎么样。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c28e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, X, Y, C, epoch_num, kernel, eps = 1e-2, tol = 1e-6, degree = 3, sigma = 1):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.n, self.p = X.shape\n",
    "        self.C = C # 惩罚系数\n",
    "        self.epoch_num = epoch_num # 最大下降次数\n",
    "        self.eps = eps # 最大违反KKT条件的程度\n",
    "        self.tol = tol # 目标函数最小下降程度\n",
    "        self.alpha = np.zeros(self.n) \n",
    "        self.b = 0\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.sigma = sigma\n",
    "        self.K = self.compute_K(self.X, self.X,) # 核变换后的内积结果\n",
    "        self.g = self.compute_g() # 对每个xi的标签的预测\n",
    "        self.E = self.g - self.Y  # 预测误差\n",
    "    @staticmethod\n",
    "    def linear_kernel(x,y):\n",
    "        return np.dot(x,y) # 换核函数只需要修改它即可\n",
    "    def poly_kernel(self,x,y):\n",
    "        return (np.dot(x,y)+1) ** self.degree\n",
    "    def gaussian_kernel(self,x,y):\n",
    "        return np.exp(-np.sum((x-y)**2)/(2 * self.sigma **2))\n",
    "    # 计算核函数内积矩阵\n",
    "    def compute_K(self, X1, X2):\n",
    "        n1 = X1.shape[0]\n",
    "        n2 = X2.shape[0]\n",
    "        K = np.zeros((n1, n2))\n",
    "        for i in range(n1):\n",
    "            for j in range(n2):\n",
    "                if self.kernel == \"linear\":\n",
    "                    K[i,j] = self.linear_kernel(X1[i],X2[j])\n",
    "                elif self.kernel == \"polynomial\":\n",
    "                    K[i,j] = self.poly_kernel(X1[i],X2[j])\n",
    "                elif self.kernel == \"gaussian\":\n",
    "                    K[i,j] = self.gaussian_kernel(X1[i],X2[j])\n",
    "        return K\n",
    "    # 计算g(x)\n",
    "    def compute_g(self):\n",
    "        g = np.zeros(self.n)\n",
    "        for i in range(self.n):\n",
    "            g[i] = np.dot(self.alpha * self.Y, self.K[i]) + self.b\n",
    "        return g\n",
    "    # 计算目标函数值\n",
    "    def compute_obj(self,alpha):\n",
    "        return np.dot(alpha*self.Y, self.K).dot(alpha* self.Y)/2 - np.sum(alpha)\n",
    "    # 外循环中按顺序寻找i1，最先搜索支持向量，再搜索远离分隔平面的点，最后搜索软间隔中的点\n",
    "    def choose_i1(self, flag1):\n",
    "        value = self.Y * self.E\n",
    "        is_support = (self.alpha > 0) & (self.alpha < self.C) # 支持向量\n",
    "        is_beyond_support = self.alpha == 0 # 远离分隔平面\n",
    "        is_between_support = self.alpha == self.C # 软间隔之间\n",
    "        not_zero = ((value < -self.eps) | (value > self.eps)) # 对支持向量而言，value不为0违反KKT条件\n",
    "        is_positive = value > self.eps # 对软间隔之间的点而言，value大于0违反KKT条件\n",
    "        is_negative = value < -self.eps # 对远离分隔平面的点而言，value小于0违反KKT条件\n",
    "        support_vio = is_support & not_zero\n",
    "        beyond_support_vio = is_beyond_support & is_negative\n",
    "        between_support_vio = is_between_support & is_positive\n",
    "        flag2 = flag1 - np.sum(support_vio)\n",
    "        flag3 = flag1 - np.sum(support_vio) - np.sum(beyond_support_vio)\n",
    "        if np.sum(support_vio) >= flag1:\n",
    "            tmp = np.sort(np.abs(value[support_vio]))\n",
    "            i1 = np.where((np.abs(value) == tmp[-flag1]) & is_support)[0]\n",
    "        elif np.sum(beyond_support_vio) >= flag2:\n",
    "            tmp = np.sort(value[beyond_support_vio])[::-1]\n",
    "            i1 = np.where((value == tmp[-flag2]) & is_beyond_support)[0]\n",
    "        elif np.sum(between_support_vio) > flag3:\n",
    "            print(前面两种都没有)\n",
    "            tmp = np.sort(value[between_support_vio])\n",
    "            i1 = np.where((value == tmp[-flag3]) & is_between_support)[0]\n",
    "        else:\n",
    "            i1 = None\n",
    "        return i1\n",
    "    # 内循环启发式寻找i2，这里将i2先排好序，用于一个一个找\n",
    "    def choose_i2(self,i1):\n",
    "        i2 = np.zeros(self.n,dtype = int)\n",
    "        idx = np.arange(self.n)\n",
    "        if self.E[i1] > 0:\n",
    "            i2[0] = np.argmin(self.E)\n",
    "        else:\n",
    "            i2[0] = np.argmax(self.E)\n",
    "        tmp1 = idx[(self.alpha > 0) & (self.alpha < self.C) & (idx != i1)]\n",
    "        tmp2 = idx[((self.alpha == 0) | (self.alpha == self.C)) & (idx != i1)]\n",
    "        i2[1:] = np.concatenate([tmp1,tmp2])\n",
    "        return i2\n",
    "        \n",
    "    def smo(self):\n",
    "        for epoch in range(self.epoch_num):\n",
    "#             print(f\"--------------------第{epoch}次迭代-----------------\")\n",
    "            out_loop = True\n",
    "            flag = 1 # 用于指明i1已经搜索到哪里了\n",
    "            obj = self.compute_obj(self.alpha)\n",
    "            while out_loop:\n",
    "                i1 = self.choose_i1(flag)\n",
    "                if type(i1) is int:\n",
    "                    i1 = [i1]\n",
    "                # i1已经找遍，且没有再能够使函数可观下降的一对alpha，所以结束算法\n",
    "                elif i1 is None:\n",
    "                    return self.alpha\n",
    "                # 外循环搜索i1\n",
    "                for j in i1:\n",
    "                    i2 = self.choose_i2(j)\n",
    "                    # 内循环搜索i2\n",
    "                    for k in i2:\n",
    "                        eta = self.K[j,j] + self.K[k,k] - 2 * self.K[j,k]\n",
    "                        alpha2_unc = self.alpha[k] + self.Y[k] * (self.E[j] - self.E[k]) / eta\n",
    "                        if self.Y[j] == self.Y[k]:\n",
    "                            L = max(0, self.alpha[j] + self.alpha[k] - self.C)\n",
    "                            H = min(self.C, self.alpha[j] + self.alpha[k])\n",
    "                        else:\n",
    "                            L = max(0, self.alpha[k] - self.alpha[j])\n",
    "                            H = min(self.C, self.C + self.alpha[k] - self.alpha[j])\n",
    "                        if alpha2_unc > H:\n",
    "                            alpha2 = H\n",
    "                        elif alpha2_unc >= L:\n",
    "                            alpha2 = alpha2_unc\n",
    "                        else:\n",
    "                            alpha2 = L\n",
    "                        alpha1 = self.alpha[j] + self.Y[j]*self.Y[k]*(self.alpha[k] - alpha2)\n",
    "                        # alpha必须都大于0，否则不更新\n",
    "                        if (alpha1 < 0) | (alpha2 < 0):\n",
    "                            continue\n",
    "                        alpha_new = self.alpha.copy()\n",
    "                        alpha_new[j] = alpha1\n",
    "                        alpha_new[k] = alpha2\n",
    "                        obj_new = self.compute_obj(alpha_new)\n",
    "                        # 目标函数是否可观下降\n",
    "                        if (obj - obj_new < self.tol):\n",
    "                            continue\n",
    "                        else:\n",
    "                            break\n",
    "                    else:\n",
    "                        flag += 1\n",
    "                        continue\n",
    "                    out_loop = False\n",
    "                    break\n",
    "            b1 = (-self.E[j] - self.Y[j] * self.K[j,j] * (alpha1 - self.alpha[j]) \n",
    "                    - self.Y[k] * self.K[k,j]* (alpha2 - self.alpha[k]) + self.b)\n",
    "            b2 = (-self.E[k] - self.Y[j] * self.K[j,k] * (alpha1 - self.alpha[j]) \n",
    "                    - self.Y[k] * self.K[k,k]* (alpha2 - self.alpha[k]) + self.b)\n",
    "            self.b = (b1+b2)/2\n",
    "            self.alpha[j] = alpha1\n",
    "            self.alpha[k] = alpha2\n",
    "            self.g = self.compute_g() \n",
    "            self.E = self.g - self.Y \n",
    "    def predict(self, newdata, thereshold = 1e-4):\n",
    "        idx = self.alpha != 0\n",
    "        K = self.compute_K(self.X[idx],newdata)\n",
    "        tmp1 = self.alpha * self.Y\n",
    "        tmp2 = np.dot(tmp1[idx],K) + self.b\n",
    "        return np.where(tmp2>=thereshold, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23f7071e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.935672514619883"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[y_train == 0] = -1\n",
    "y_test[y_test == 0] = -1\n",
    "svm = SVM(Y = y_train,\n",
    "          X = X_train,\n",
    "          C = 1,\n",
    "          epoch_num = 100,\n",
    "          kernel = \"linear\")\n",
    "svm.smo()\n",
    "np.mean(svm.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00159afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVM(Y = y_train,\n",
    "          X = X_train,\n",
    "          C = 1,\n",
    "          epoch_num = 100,\n",
    "          kernel = \"gaussian\",\n",
    "          sigma = 7.5)\n",
    "svm.smo()\n",
    "np.mean(svm.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d822f6e",
   "metadata": {},
   "source": [
    "结果挺奇怪的，可能代码里有bug。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7c00988",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[y_train == -1] = 0\n",
    "y_test[y_test == -1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da596e6",
   "metadata": {},
   "source": [
    "## 树模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b2f369",
   "metadata": {},
   "source": [
    "### 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79d30cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self, Y, X, min_node_size, max_node_depth, rule = None, threshold = 1e-4):\n",
    "        # 数据存储\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n, self.feature_num = X.shape\n",
    "        # 递归控制\n",
    "        self.gini = self.compute_gini(Y)\n",
    "        self.min_node_size = min_node_size\n",
    "        self.max_node_depth = max_node_depth\n",
    "        # 分裂规则\n",
    "        self.best_feature = None\n",
    "        self.best_value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        # 节点预测\n",
    "        self.threshold = 1e-4\n",
    "        self.ybar = np.mean(Y)\n",
    "        self.yhat = 1 if self.ybar >= (0.5+self.threshold) else 0\n",
    "        # 规则记录\n",
    "        self.rule = rule if rule is not None else \"\"\n",
    "    @staticmethod\n",
    "    def compute_gini(y):\n",
    "        n0 = np.sum(y == 0)\n",
    "        n1 = np.sum(y == 1)\n",
    "        if (n1 == 0) or (n0 == 0):\n",
    "            return 0\n",
    "        else:\n",
    "            n = len(y)\n",
    "            p0 = n0/n\n",
    "            p1 = n1/n\n",
    "            gini_index = 1-(p0**2 + p1**2)\n",
    "            return gini_index\n",
    "    def split_find(self):\n",
    "        max_gain = 0\n",
    "        best_feature = None\n",
    "        best_value = None\n",
    "        for feature in range(self.feature_num):\n",
    "            grid = np.sort(np.unique(self.X[:,feature]))\n",
    "            for value in grid:\n",
    "                lidx = self.X[:,feature] < value\n",
    "                ridx = self.X[:,feature] >= value\n",
    "                n_left = np.sum(lidx)\n",
    "                n_right = np.sum(ridx)\n",
    "                if (n_left == 0) or (n_right == 0):\n",
    "                    continue\n",
    "                gini_left = self.compute_gini(self.Y[lidx])\n",
    "                gini_right = self.compute_gini(self.Y[ridx])\n",
    "                gini_gain = self.gini - (n_left * gini_left + n_right * gini_right)/self.n\n",
    "                if gini_gain > max_gain:\n",
    "                    best_feature = feature\n",
    "                    best_value = value\n",
    "                    max_gain = gini_gain\n",
    "        return (best_feature,best_value)\n",
    "    def fit(self):\n",
    "        if (self.max_node_depth > 0) and (self.min_node_size < self.n) and (self.gini > 0):\n",
    "            self.best_feature,self.best_value = self.split_find()\n",
    "            if self.best_feature is not None:\n",
    "                lidx = self.X[:,self.best_feature] < self.best_value\n",
    "                ridx = self.X[:,self.best_feature] >= self.best_value\n",
    "                left = Tree(Y = self.Y[lidx],\n",
    "                            X = self.X[lidx,:],\n",
    "                            min_node_size = self.min_node_size,\n",
    "                            max_node_depth = self.max_node_depth - 1,\n",
    "                            rule = f\"X{self.best_feature+1} < {self.best_value}\")\n",
    "                right = Tree(Y = self.Y[ridx],\n",
    "                             X = self.X[ridx,:],\n",
    "                             min_node_size = self.min_node_size,\n",
    "                             max_node_depth = self.max_node_depth - 1,\n",
    "                             rule = f\"X{self.best_feature+1} >= {self.best_value}\")\n",
    "                left.fit()\n",
    "                right.fit()\n",
    "                self.left = left\n",
    "                self.right = right\n",
    "    def predict_prob(self,newdata):\n",
    "        n = newdata.shape[0]\n",
    "        yhat = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            cur_node = self\n",
    "            if (cur_node.max_node_depth > 0) and (cur_node.min_node_size < cur_node.n) and (cur_node.gini > 0):\n",
    "                if cur_node.best_feature is None:\n",
    "                    break\n",
    "                if newdata[i,cur_node.best_feature] < cur_node.best_value:\n",
    "                    cur_node = cur_node.left\n",
    "                else:\n",
    "                    cur_node = cur_node.right\n",
    "            yhat[i] = cur_node.ybar\n",
    "        return yhat\n",
    "    def predict(self,newdata):\n",
    "        n = newdata.shape[0]\n",
    "        yhat = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            cur_node = self\n",
    "            while (cur_node.max_node_depth > 0) and (cur_node.min_node_size < cur_node.n) and (cur_node.gini > 0):\n",
    "                if cur_node.best_feature is None:\n",
    "                    break\n",
    "                if newdata[i,cur_node.best_feature] < cur_node.best_value:\n",
    "                    cur_node = cur_node.left\n",
    "                else:\n",
    "                    cur_node = cur_node.right\n",
    "            yhat[i] = cur_node.yhat\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "813b1a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.935672514619883"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = Tree(Y = y_train,\n",
    "            X = X_train,\n",
    "            min_node_size = 10,\n",
    "            max_node_depth = 5)\n",
    "tree.fit()\n",
    "np.mean(tree.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67e772",
   "metadata": {},
   "source": [
    "接下来看集成模型的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac2bf6",
   "metadata": {},
   "source": [
    "### bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33b903",
   "metadata": {},
   "source": [
    "#### 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdd0a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self, Y, X, min_node_size, max_node_depth, mtry, rule = None, threshold = 1e-4):\n",
    "        # 数据存储\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n, self.feature_num = X.shape\n",
    "        # 递归控制\n",
    "        self.gini = self.compute_gini(Y)\n",
    "        self.min_node_size = min_node_size\n",
    "        self.max_node_depth = max_node_depth\n",
    "        # 分裂规则\n",
    "        self.best_feature = None\n",
    "        self.best_value = None\n",
    "        self.mtry = mtry\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        # 节点预测\n",
    "        self.threshold = 1e-4\n",
    "        self.ybar = np.mean(Y)\n",
    "        self.yhat = 1 if self.ybar >= (0.5+self.threshold) else 0\n",
    "        # 规则记录\n",
    "        self.rule = rule if rule is not None else \"\"\n",
    "    @staticmethod\n",
    "    def compute_gini(y):\n",
    "        n0 = np.sum(y == 0)\n",
    "        n1 = np.sum(y == 1)\n",
    "        if (n1 == 0) or (n0 == 0):\n",
    "            return 0\n",
    "        else:\n",
    "            n = len(y)\n",
    "            p0 = n0/n\n",
    "            p1 = n1/n\n",
    "            gini_index = 1-(p0**2 + p1**2)\n",
    "            return gini_index\n",
    "    def split_find(self):\n",
    "        max_gain = 0\n",
    "        best_feature = None\n",
    "        best_value = None\n",
    "        feature_try = np.random.choice(np.arange(self.feature_num), self.mtry, replace = False)\n",
    "        for feature in feature_try:\n",
    "            grid = np.sort(np.unique(self.X[:,feature]))\n",
    "            for value in grid:\n",
    "                lidx = self.X[:,feature] < value\n",
    "                ridx = self.X[:,feature] >= value\n",
    "                n_left = np.sum(lidx)\n",
    "                n_right = np.sum(ridx)\n",
    "                if (n_left == 0) or (n_right == 0):\n",
    "                    continue\n",
    "                gini_left = self.compute_gini(self.Y[lidx])\n",
    "                gini_right = self.compute_gini(self.Y[ridx])\n",
    "                gini_gain = self.gini - (n_left * gini_left + n_right * gini_right)/self.n\n",
    "                if gini_gain > max_gain:\n",
    "                    best_feature = feature\n",
    "                    best_value = value\n",
    "                    max_gain = gini_gain\n",
    "        return (best_feature,best_value)\n",
    "    def fit(self):\n",
    "        if (self.max_node_depth > 0) and (self.min_node_size < self.n) and (self.gini > 0):\n",
    "            self.best_feature,self.best_value = self.split_find()\n",
    "            if self.best_feature is not None:\n",
    "                lidx = self.X[:,self.best_feature] < self.best_value\n",
    "                ridx = self.X[:,self.best_feature] >= self.best_value\n",
    "                left = Tree(Y = self.Y[lidx],\n",
    "                            X = self.X[lidx,:],\n",
    "                            min_node_size = self.min_node_size,\n",
    "                            max_node_depth = self.max_node_depth - 1,\n",
    "                            mtry = self.mtry,\n",
    "                            rule = f\"X{self.best_feature+1} < {self.best_value}\")\n",
    "                right = Tree(Y = self.Y[ridx],\n",
    "                             X = self.X[ridx,:],\n",
    "                             min_node_size = self.min_node_size,\n",
    "                             max_node_depth = self.max_node_depth - 1,\n",
    "                             mtry = self.mtry,\n",
    "                             rule = f\"X{self.best_feature+1} >= {self.best_value}\")\n",
    "                left.fit()\n",
    "                right.fit()\n",
    "                self.left = left\n",
    "                self.right = right\n",
    "    def predict_prob(self,newdata):\n",
    "        n = newdata.shape[0]\n",
    "        yhat = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            cur_node = self\n",
    "            if (cur_node.max_node_depth > 0) and (cur_node.min_node_size < cur_node.n) and (cur_node.gini > 0):\n",
    "                if cur_node.best_feature is None:\n",
    "                    break\n",
    "                if newdata[i,cur_node.best_feature] < cur_node.best_value:\n",
    "                    cur_node = cur_node.left\n",
    "                else:\n",
    "                    cur_node = cur_node.right\n",
    "            yhat[i] = cur_node.ybar\n",
    "        return yhat\n",
    "    def predict(self,newdata):\n",
    "        n = newdata.shape[0]\n",
    "        yhat = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            cur_node = self\n",
    "            while (cur_node.max_node_depth > 0) and (cur_node.min_node_size < cur_node.n) and (cur_node.gini > 0):\n",
    "                if cur_node.best_feature is None:\n",
    "                    break\n",
    "                if newdata[i,cur_node.best_feature] < cur_node.best_value:\n",
    "                    cur_node = cur_node.left\n",
    "                else:\n",
    "                    cur_node = cur_node.right\n",
    "            yhat[i] = cur_node.yhat\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ee4d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "class RandomForest:\n",
    "    def __init__(self, Y, X, min_node_size, max_node_depth, ntree, mtry):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n = X.shape[0]\n",
    "        self.min_node_size = min_node_size\n",
    "        self.max_node_depth = max_node_depth\n",
    "        self.ntree = ntree\n",
    "        self.mtry = mtry\n",
    "        self.tree_set = []\n",
    "    def fit(self):\n",
    "        for i in tqdm(range(self.ntree)):\n",
    "            bootstrap_index = np.random.choice(np.arange(self.n), self.n, replace = True)\n",
    "            X = self.X[bootstrap_index,]\n",
    "            Y = self.Y[bootstrap_index]\n",
    "            tree = Tree(Y = Y,\n",
    "                        X = X,\n",
    "                        min_node_size = self.min_node_size,\n",
    "                        max_node_depth = self.max_node_depth,\n",
    "                        mtry = self.mtry)\n",
    "            tree.fit()\n",
    "            self.tree_set.append(tree)\n",
    "    def predict(self, newdata, threshold = 1e-4):\n",
    "        n = newdata.shape[0]\n",
    "        tmp = np.zeros((self.ntree,n))\n",
    "        for i in range(self.ntree):\n",
    "            tmp[i,] = self.tree_set[i].predict(newdata)\n",
    "        vote_mean = np.mean(tmp, axis = 0)\n",
    "        return np.where(vote_mean > (0.5+threshold), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ae9c896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  3.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9824561403508771"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1118)\n",
    "rf = RandomForest(Y = y_train,\n",
    "                  X = X_train,\n",
    "                  min_node_size = 10,\n",
    "                  max_node_depth = 4,\n",
    "                  ntree = 10,\n",
    "                  mtry = 4)\n",
    "rf.fit()\n",
    "np.mean(rf.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c199b4",
   "metadata": {},
   "source": [
    "### boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4256df",
   "metadata": {},
   "source": [
    "#### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de70be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self, Y, X, min_node_size, max_node_depth, weight = None, rule = None):\n",
    "        # 数据\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n, self.n_feature = X.shape\n",
    "        # 递归控制\n",
    "        self.min_node_size = min_node_size\n",
    "        self.max_node_depth = max_node_depth\n",
    "        # 节点分裂\n",
    "        self.best_feature = None\n",
    "        self.best_value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        # 节点预测\n",
    "        self.yhat = self.node_predict(Y)\n",
    "        # 样本点权重\n",
    "        self.weight = weight if weight is not None else np.ones(self.n)/self.n\n",
    "        # 节点误差率\n",
    "        self.err = self.compute_err(Y, self.yhat, self.weight)\n",
    "        # 规则记录\n",
    "        self.rule = rule if rule is not None else \"\"\n",
    "    @staticmethod\n",
    "    def node_predict(y, threshold = 1e-4):\n",
    "        tmp = np.mean(y)\n",
    "        yhat = 1 if tmp > threshold else -1\n",
    "        return yhat\n",
    "    @staticmethod\n",
    "    def compute_err(y, yhat, weight):\n",
    "        return np.dot((y != yhat).astype(int), weight)\n",
    "    def split_find(self):\n",
    "        best_feature = None\n",
    "        best_value = None\n",
    "        max_gain = 0\n",
    "        for feature in range(self.n_feature):\n",
    "            grid = np.sort(np.unique(self.X[:,feature]))\n",
    "            for value in grid:\n",
    "                lidx = self.X[:,feature] < value\n",
    "                ridx = self.X[:,feature] >= value\n",
    "                if (np.sum(lidx) == 0) or (np.sum(ridx) == 0):\n",
    "                    continue\n",
    "                y_left = self.Y[lidx]\n",
    "                y_right = self.Y[ridx]\n",
    "                w_left = self.weight[lidx]\n",
    "                w_right = self.weight[ridx]\n",
    "                err_new = (self.compute_err(y_left, self.node_predict(y_left), w_left)+\n",
    "                          self.compute_err(y_right, self.node_predict(y_right), w_right))\n",
    "                err_gain = self.err - err_new\n",
    "                if err_gain > max_gain:\n",
    "                    best_feature = feature\n",
    "                    best_value = value\n",
    "                    max_gain = err_gain\n",
    "        return (best_feature, best_value)\n",
    "    def fit(self):\n",
    "        if (self.min_node_size < self.n) and (self.max_node_depth > 0) and (self.err > 0):\n",
    "            self.best_feature,self.best_value = self.split_find()\n",
    "            if self.best_feature is not None:\n",
    "                lidx = self.X[:,self.best_feature] < self.best_value\n",
    "                ridx = self.X[:,self.best_feature] >= self.best_value\n",
    "                left = Tree(X = self.X[lidx,],\n",
    "                            Y = self.Y[lidx],\n",
    "                            min_node_size = self.min_node_size,\n",
    "                            max_node_depth = self.max_node_depth - 1,\n",
    "                            weight = self.weight[lidx],\n",
    "                            rule = f\"X{self.best_feature+1} < {self.best_value}\")\n",
    "                right = Tree(X = self.X[ridx,],\n",
    "                             Y = self.Y[ridx],\n",
    "                             min_node_size = self.min_node_size,\n",
    "                             max_node_depth = self.max_node_depth - 1,\n",
    "                             weight = self.weight[ridx],\n",
    "                             rule = f\"X{self.best_feature+1} >= {self.best_value}\")\n",
    "                left.fit()\n",
    "                right.fit()\n",
    "                self.left = left\n",
    "                self.right = right\n",
    "    def predict(self, newdata):\n",
    "        n = newdata.shape[0]\n",
    "        yhat = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            cur_node = self\n",
    "            while (cur_node.max_node_depth > 0) and (cur_node.min_node_size < cur_node.n) and (cur_node.err > 0):\n",
    "                if cur_node.best_feature is None:\n",
    "                    break\n",
    "                if newdata[i,cur_node.best_feature] < cur_node.best_value:\n",
    "                    cur_node = cur_node.left\n",
    "                else:\n",
    "                    cur_node = cur_node.right\n",
    "            yhat[i] = cur_node.yhat\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2fb149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaboost:\n",
    "    def __init__(self, Y, X, min_node_size, max_node_depth, nboost):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n = X.shape[0]\n",
    "        self.min_node_size = min_node_size\n",
    "        self.max_node_depth = max_node_depth\n",
    "        self.nboost = nboost\n",
    "        self.tree_set = []\n",
    "        self.alpha = []\n",
    "    @staticmethod\n",
    "    def compute_err(y, yhat, weight):\n",
    "        return np.dot((y != yhat).astype(int), weight)\n",
    "    def fit(self):\n",
    "        weight = np.ones(self.n)/self.n\n",
    "        for i in tqdm(range(self.nboost)):\n",
    "            tree = Tree(X = self.X,\n",
    "                        Y = self.Y,\n",
    "                        min_node_size = self.min_node_size,\n",
    "                        max_node_depth = self.max_node_depth,\n",
    "                        weight = weight)\n",
    "            tree.fit()\n",
    "            yhat = tree.predict(self.X)\n",
    "            err = self.compute_err(self.Y, yhat, weight)\n",
    "            alpha = np.log((1-err)/err)/2\n",
    "            w = weight * np.exp(-alpha * self.Y * yhat)\n",
    "            weight = w/np.sum(w)\n",
    "            self.tree_set.append(tree)\n",
    "            self.alpha.append(alpha)\n",
    "    def predict(self, newdata, threshold = 1e-4):\n",
    "        n = newdata.shape[0]\n",
    "        cum = np.zeros(n)\n",
    "        for i in range(self.nboost):\n",
    "            cum += self.alpha[i] * self.tree_set[i].predict(newdata)\n",
    "        return np.where(cum > threshold, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "730e58be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:24<00:00,  2.47s/it]\n"
     ]
    }
   ],
   "source": [
    "y_train[y_train == 0] = -1\n",
    "y_test[y_test == 0] = -1\n",
    "adaboost = Adaboost(X = X_train,\n",
    "                    Y = y_train,\n",
    "                    min_node_size = 10,\n",
    "                    max_node_depth = 4,\n",
    "                    nboost = 10)\n",
    "adaboost.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f9c3454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9707602339181286"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(adaboost.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d21a41",
   "metadata": {},
   "source": [
    "这次结果确实好看，不过训练速度有点慢。。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e36857",
   "metadata": {},
   "source": [
    "#### GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a604437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBDT的基学习器\n",
    "class Tree:\n",
    "    def __init__(self, Y, X, P, min_node_size, max_node_depth, rule = None):\n",
    "        # 数据存储\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.P = P\n",
    "        self.n, self.feature_num = X.shape\n",
    "        # 递归控制\n",
    "        self.min_node_size = min_node_size\n",
    "        self.max_node_depth = max_node_depth\n",
    "        self.n = len(Y)\n",
    "        # 节点分裂信息\n",
    "        self.mse = np.var(Y)\n",
    "        self.best_feature = None\n",
    "        self.best_value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        # 节点预测值\n",
    "        self.yhat = np.sum(Y)/np.sum(P * (1-P))\n",
    "        # 字符串记录，可以用来调试\n",
    "        self.rule = rule if rule is not None else \"\"\n",
    "    def split_find(self):\n",
    "        max_gain = 0\n",
    "        best_feature = None\n",
    "        best_value = None\n",
    "        for feature in range(self.feature_num):\n",
    "            grid = np.sort(np.unique(self.X[:,feature]))\n",
    "            for value in grid:\n",
    "                lidx = self.X[:, feature] < value\n",
    "                ridx = self.X[:, feature] >= value\n",
    "                n_left = np.sum(lidx)\n",
    "                n_right = np.sum(ridx)\n",
    "                # 注意这里只是跳出本次循环\n",
    "                if (n_left == 0) or (n_right == 0):\n",
    "                    continue\n",
    "                mse_left = np.var(self.Y[lidx])\n",
    "                mse_right = np.var(self.Y[ridx])\n",
    "                mse_gain = self.mse - (n_left * mse_left + n_right * mse_right) / self.n\n",
    "                if mse_gain > max_gain:\n",
    "                    best_feature = feature\n",
    "                    best_value = value\n",
    "                    max_gain = mse_gain\n",
    "        return (best_feature, best_value)\n",
    "    def fit(self):\n",
    "        if (self.max_node_depth > 0) and (self.min_node_size < self.n):\n",
    "            self.best_feature,self.best_value = self.split_find()\n",
    "            if self.best_feature is not None:\n",
    "                lidx = self.X[:, self.best_feature] < self.best_value\n",
    "                ridx = self.X[:, self.best_feature] >= self.best_value\n",
    "                left = Tree(Y = self.Y[lidx],\n",
    "                            X = self.X[lidx],\n",
    "                            P = self.P[lidx],\n",
    "                            min_node_size = self.min_node_size,\n",
    "                            max_node_depth = self.max_node_depth - 1,\n",
    "                            rule = f\"X{self.best_feature+1} < {self.best_value}\")\n",
    "                left.fit()\n",
    "                right = Tree(Y = self.Y[ridx],\n",
    "                             X = self.X[ridx],\n",
    "                             P = self.P[ridx],\n",
    "                             min_node_size = self.min_node_size,\n",
    "                             max_node_depth = self.max_node_depth - 1,\n",
    "                             rule = f\"X{self.best_feature+1} >= {self.best_value}\")\n",
    "                right.fit()\n",
    "                self.left = left\n",
    "                self.right = right\n",
    "    def predict(self,newdata):\n",
    "        n = newdata.shape[0]\n",
    "        yhat = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            cur_node = self\n",
    "            while (cur_node.max_node_depth > 0) and (cur_node.min_node_size < cur_node.n):\n",
    "                if cur_node.best_feature is None:\n",
    "                    break\n",
    "                if newdata[i,cur_node.best_feature] < cur_node.best_value:\n",
    "                    cur_node = cur_node.left\n",
    "                else:\n",
    "                    cur_node = cur_node.right\n",
    "            yhat[i] = cur_node.yhat\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8ab8785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "class GBDTclassifier:\n",
    "    def __init__(self, Y, X, min_node_size, max_node_depth, nboost, lr = 0.5):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.n = X.shape[0]\n",
    "        self.min_node_size = min_node_size\n",
    "        self.max_node_depth = max_node_depth\n",
    "        self.nboost = nboost\n",
    "        self.lr = lr\n",
    "        self.log_odds = np.log(np.sum(Y)/np.sum(1-Y))\n",
    "        self.tree_set = []\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    def fit(self):\n",
    "        yhat = np.ones(self.n) * self.log_odds\n",
    "        for i in tqdm(range(self.nboost)):\n",
    "            P = self.sigmoid(yhat)\n",
    "            res = self.Y - P\n",
    "            tree = Tree(Y = res,\n",
    "                        X = self.X,\n",
    "                        P = P,\n",
    "                        min_node_size = self.min_node_size,\n",
    "                        max_node_depth = self.max_node_depth)\n",
    "            tree.fit()\n",
    "            yhat += tree.predict(self.X) * self.lr\n",
    "            self.tree_set.append(tree)\n",
    "    def predict_prob(self, newdata):\n",
    "        p = self.log_odds.copy()\n",
    "        for tree in self.tree_set:\n",
    "            p += tree.predict(newdata) * self.lr\n",
    "        return p\n",
    "    def predict(self, newdata, threshold = 1e-4):\n",
    "        p = self.predict_prob(newdata)\n",
    "        return np.where(p >= (0.5+threshold), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5803a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X,y = datasets.load_breast_cancer(return_X_y=True)\n",
    "# 划分训练集和测试集\n",
    "np.random.seed(104)\n",
    "index1 = np.random.choice(X.shape[0],round(X.shape[0]*0.7),replace = False)\n",
    "index2 = np.array(list(set(range(X.shape[0]))-set(index1)))\n",
    "X_train, X_test, y_train, y_test = X[index1,].copy(), X[index2,].copy(), y[index1].copy(), y[index2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a83c54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:36<00:00,  3.61s/it]\n"
     ]
    }
   ],
   "source": [
    "gbdt = GBDTclassifier(Y = y_train,\n",
    "                      X = X_train,\n",
    "                      min_node_size = 10,\n",
    "                      max_node_depth = 4,\n",
    "                      nboost = 10,\n",
    "                      lr = 0.7)\n",
    "gbdt.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ac152ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9766081871345029"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(gbdt.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ffc74c",
   "metadata": {},
   "source": [
    "#### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b3dc7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这次尝试一下X为ndarray\n",
    "class Tree:\n",
    "    def __init__(self, X, grad, hess, min_node_size, max_node_depth, mtry, lambda_ = 1, gamma = 1, \n",
    "                 eps = 0.1, rule = None):\n",
    "        self.X = X\n",
    "        self.n, self.feature_num = X.shape\n",
    "        self.grad = grad\n",
    "        self.hess = hess\n",
    "        self.min_node_size = min_node_size\n",
    "        self.max_node_depth = max_node_depth\n",
    "        self.mtry = mtry\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.score = self.compute_score(self.grad, self.hess) #叶节点得分\n",
    "        self.rule = rule if rule is not None else \"\"\n",
    "        self.best_feature = None\n",
    "        self.best_value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.leaf_weight = self.compute_weight(self.grad, self.hess)\n",
    "    def compute_score(self, grad, hess):\n",
    "        return -1/2 * np.sum(grad)**2 / (np.sum(hess) + self.lambda_)\n",
    "    def compute_weight(self, grad, hess):\n",
    "        return - np.sum(grad) / (np.sum(hess) + self.lambda_)\n",
    "    def split_find(self):\n",
    "        max_gain = 0\n",
    "        best_feature = None\n",
    "        best_value = None\n",
    "        # Exact Greedy Algorithm\n",
    "        feature_try = np.random.choice(np.arange(self.feature_num), self.mtry, replace = False)\n",
    "        for feature in feature_try:\n",
    "            grid = np.sort(np.unique(self.X[:,feature]))\n",
    "            for value in grid:\n",
    "                lidx = self.X[:, feature] < value\n",
    "                ridx = self.X[:, feature] >= value\n",
    "                lscore = self.compute_score(self.grad[lidx], self.hess[lidx])\n",
    "                rscore = self.compute_score(self.grad[ridx], self.hess[ridx])\n",
    "                score_gain = (self.score - lscore - rscore)/2 - self.gamma\n",
    "                if score_gain > max_gain:\n",
    "                    best_feature = feature\n",
    "                    best_value = value\n",
    "                    max_gain = score_gain\n",
    "        return (best_feature, best_value)\n",
    "    def grow_tree(self):\n",
    "        if (self.max_node_depth > 0) and (self.min_node_size < self.n):\n",
    "            self.best_feature,self.best_value = self.split_find()\n",
    "            if self.best_feature is not None:\n",
    "                lidx = self.X[:, self.best_feature] < self.best_value\n",
    "                ridx = self.X[:, self.best_feature] >= self.best_value\n",
    "                left = Tree(X = self.X[lidx],\n",
    "                            grad = self.grad[lidx],\n",
    "                            hess = self.hess[lidx],\n",
    "                            min_node_size = self.min_node_size,\n",
    "                            max_node_depth = self.max_node_depth - 1,\n",
    "                            mtry = self.mtry,\n",
    "                            lambda_ = self.lambda_,\n",
    "                            gamma = self.gamma,\n",
    "                            eps = self.eps,\n",
    "                            rule = f\"X{self.best_feature+1} < {self.best_value}\")\n",
    "                left.grow_tree()\n",
    "                right = Tree(X = self.X[ridx],\n",
    "                            grad = self.grad[ridx],\n",
    "                            hess = self.hess[ridx],\n",
    "                            min_node_size = self.min_node_size,\n",
    "                            max_node_depth = self.max_node_depth - 1,\n",
    "                            mtry = self.mtry,\n",
    "                            lambda_ = self.lambda_,\n",
    "                            gamma = self.gamma,\n",
    "                            eps = self.eps,\n",
    "                            rule = f\"X{self.best_feature+1} >= {self.best_value}\")\n",
    "                right.grow_tree()\n",
    "                self.left = left\n",
    "                self.right = right\n",
    "    def predict(self, newdata):\n",
    "        n = newdata.shape[0]\n",
    "        yhat = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            cur_node = self\n",
    "            while (cur_node.max_node_depth > 0) and (cur_node.min_node_size < cur_node.n):\n",
    "                if cur_node.best_feature is None:\n",
    "                    break\n",
    "                if newdata[i,cur_node.best_feature] < cur_node.best_value:\n",
    "                    cur_node = cur_node.left\n",
    "                else:\n",
    "                    cur_node = cur_node.right\n",
    "            yhat[i] = cur_node.leaf_weight\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49990dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XgboostClassifier:\n",
    "    def __init__(self, X, Y, min_node_size, max_node_depth, mtry, nboost, lambda_ = 1, gamma = 1, eps = 0.1, lr = 1):\n",
    "        self.X = X\n",
    "        self.n,self.p = X.shape\n",
    "        self.Y = Y\n",
    "        self.min_node_size = min_node_size\n",
    "        self.max_node_depth = max_node_depth\n",
    "        self.mtry = mtry\n",
    "        self.nboost = nboost\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.lr = lr\n",
    "        self.tree_set = []\n",
    "        self.base_pred = np.ones_like(Y) * self.log_odds(Y)\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    @staticmethod\n",
    "    def log_odds(y):\n",
    "        odds = np.sum(y == 1)/np.sum(y == 0)\n",
    "        return np.log(odds)\n",
    "    \n",
    "    def fit(self):\n",
    "        for i in tqdm(range(self.nboost)):\n",
    "            p = self.sigmoid(self.base_pred)\n",
    "            grad = p - self.Y\n",
    "            hess = p * (1 - p)\n",
    "            tree = Tree(X = self.X,\n",
    "                        grad = grad,\n",
    "                        hess = hess,\n",
    "                        min_node_size = self.min_node_size,\n",
    "                        max_node_depth = self.max_node_depth,\n",
    "                        mtry = self.mtry,\n",
    "                        lambda_ = self.lambda_,\n",
    "                        gamma = self.gamma,\n",
    "                        eps = self.eps)\n",
    "            tree.grow_tree()\n",
    "            self.base_pred += self.lr * tree.predict(self.X)\n",
    "            self.tree_set.append(tree)\n",
    "    def predict_prob(self,newdata):\n",
    "        n = newdata.shape[0]\n",
    "        yhat = np.ones(n) * self.log_odds(self.Y)\n",
    "        for tree in self.tree_set:\n",
    "            yhat += self.lr * tree.predict(newdata)\n",
    "        return self.sigmoid(yhat)\n",
    "    def predict(self, newdata, threshold = (0.5+1e-4)):\n",
    "        p = self.predict_prob(newdata)\n",
    "        pred = np.where(p >= threshold, 1, 0)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18645842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:04<00:00,  6.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9883040935672515"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1118)\n",
    "xgbooster = XgboostClassifier(X = X_train,\n",
    "                              Y = y_train,\n",
    "                              min_node_size = 10,\n",
    "                              max_node_depth = 4,\n",
    "                              mtry = 5,\n",
    "                              nboost = 30,\n",
    "                              lr = 0.7,\n",
    "                              gamma = 2)\n",
    "xgbooster.fit()\n",
    "np.mean(xgbooster.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4505ddc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "323px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
